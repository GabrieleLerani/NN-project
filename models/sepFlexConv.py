import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as f
from torchvision import datasets, transforms
from models import KGN


class SFCNN(nn.modules):
    def __init__(self, in_channels, out_channels, kernel_size):
        """
        Method to init the modified FlexConv layer, here is used the continuous kernel.
        Here a channel-wise convolution is computed with a kernel generated by
        a kernel generator network GKernel ∶ R^D →R^N_in, followed by a point-wise convolution from N_in to N_out
        """
        # TODO we generate the depthwise kernel
        self.GKernel = KGN()

        # TODO stride and padding and other conf params maybe in the yaml
        # we perform depthwise convolution must be done custom
        # because here you cannot specify the kernel
        # but the implementation must be like the following
        self.depthwise_conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            groups=in_channels,
        )
        # we perform pointwise convolution
        self.pointwise_conv = nn.Conv2d(
            out_channels=out_channels, in_channels=in_channels, kernel_size=1
        )

    def forward(self, x):
        """
        Standard method of nn.modules
        """

        # generating the kernel for each input
        K = self.GKernel(x)

        # channel-wise convolution
        x = self.depthwise_conv(x, K)

        # pointwise-convolution
        x = self.pointwise_conv(x)

        return x

    def depthwise_conv(self, x, K, stride=1, padding=0):
        """
        Custom method to perform a depthwise convolution operation with a
        pre-generated kernel coming from the GKernel function
            Args :
                - x input tensor of shape (N,C_in,H_in,W_in)
                - K kernel tensor of shape (C_in,1,K_in,K_in)

            Returns:
                - output tensor of shape (N,C_in = C_out,H_out,W_out
        """

        N, C_in, H_in, W_in = x.shape
        C_in, _, K_H_in, K_W_in = K.shape
        H_out = (H_in - K_H_in + 2 * padding) // stride + 1
        W_out = (W_in - K_W_in + 2 * padding) // stride + 1
        output_tensor = torch.zeros(N, C_in, H_out, W_out)

        for c in range(C_in):
            current_channel = x[:, c : c + 1, :, :]
            current_filter_channel = K[:, c : c + 1, :, :]
            output_channel = f.conv2d(
                current_channel, current_filter_channel, padding=1
            )
            output_tensor[:, c : c + 1, :, :] = output_channel

        return output_channel
