{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NfQnd5BX9eg"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XzWk0t-20n3Z",
        "outputId": "412314c0-e13e-456e-ebcd-8c8c01b0a482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.32.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch_lightning) (12.6.68)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install pytorch_lightning\n",
        "%pip install datasets\n",
        "%pip install hydra-core\n",
        "%pip install -U portalocker>=2.0.0\n",
        "#%pip install -r requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAIl2ZyHYAhR"
      },
      "source": [
        "General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DCZf8pQWX8qs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from typing import Optional, Callable, Tuple, Dict, List, cast\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "import requests\n",
        "\n",
        "from hydra import utils\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import logging\n",
        "import pickle\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxM4KubZfhc"
      },
      "source": [
        "# CCCN Implementation Path Finder\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ycdt1gJ00V1"
      },
      "outputs": [],
      "source": [
        "# There's an empty file in the dataset\n",
        "PATHFINDER_BLACKLIST = {\"pathfinder32/curv_baseline/imgs/0/sample_172.png\"}\n",
        "\n",
        "\n",
        "def pil_loader_grayscale(path: str) -> Image.Image:\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return Image.open(f).convert(\"L\")\n",
        "\n",
        "\n",
        "class PathFinderDataset(datasets.ImageFolder):\n",
        "    \"\"\"Path Finder dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            root,\n",
        "            loader=pil_loader_grayscale,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform,\n",
        "            is_valid_file=is_valid_file,\n",
        "        )\n",
        "\n",
        "    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "        \"\"\"Override this so it doesn't call the parent's method.\"\"\"\n",
        "        return [], {}\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataset(\n",
        "        directory: str,\n",
        "        class_to_idx: Dict[str, int],\n",
        "        extensions: Optional[Tuple[str, ...]] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "        allow_empty: bool = False\n",
        "    ) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\"\"\"\n",
        "        directory = Path(directory).expanduser()\n",
        "\n",
        "        both_none = extensions is None and is_valid_file is None\n",
        "        both_something = extensions is not None and is_valid_file is not None\n",
        "        if both_none or both_something:\n",
        "            raise ValueError(\n",
        "                \"Both extensions and is_valid_file cannot be None or not None at the same time\"\n",
        "            )\n",
        "\n",
        "        if extensions is not None:\n",
        "            def is_valid_file(x: str) -> bool:\n",
        "                return datasets.folder.has_file_allowed_extension(\n",
        "                    x, cast(Tuple[str, ...], extensions)\n",
        "                )\n",
        "        is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
        "\n",
        "        path_list = sorted(\n",
        "            list((directory / \"metadata\").glob(\"*.npy\")),\n",
        "            key=lambda path: int(path.stem),\n",
        "        )\n",
        "        if not path_list:\n",
        "            raise FileNotFoundError(f\"No metadata found at {str(directory)}\")\n",
        "        # Get the 'pathfinder32/curv_baseline' part of data_dir\n",
        "        data_dir_stem = Path().joinpath(*directory.parts[-2:])\n",
        "        instances = []\n",
        "        for metadata_file in path_list:\n",
        "            with open(metadata_file, \"r\") as f:\n",
        "                for metadata in f.read().splitlines():\n",
        "                    metadata = metadata.split()\n",
        "                    image_path = Path(metadata[0]) / metadata[1]\n",
        "                    if (\n",
        "                        is_valid_file(str(image_path))\n",
        "                        and str(data_dir_stem / image_path) not in PATHFINDER_BLACKLIST\n",
        "                    ):\n",
        "                        label = int(metadata[3])\n",
        "                        instances.append((str(directory / image_path), label))\n",
        "        return instances\n",
        "\n",
        "\n",
        "\n",
        "class PathFinderDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        num_workers,\n",
        "        pin_memory,\n",
        "        resolution,\n",
        "        level=\"hard\",\n",
        "        val_split=0.1,\n",
        "        test_split=0.1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert resolution in [32, 64, 128, 256]\n",
        "        assert level in [\"easy\", \"intermediate\", \"hard\"]\n",
        "\n",
        "        level_dir = {\n",
        "            \"easy\": \"curv_baseline\",\n",
        "            \"intermediate\": \"curv_contour_length_9\",\n",
        "            \"hard\": \"curv_contour_length_14\",\n",
        "        }[level]\n",
        "\n",
        "        # Save parameters to self\n",
        "        data_dir = (\n",
        "            data_dir + f\"/lra_release/pathfinder32/curv_contour_length_14/lra_release/lra_release/pathfinder{resolution}/{level_dir}\"\n",
        "        )\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.resolution = resolution\n",
        "        self.level = level\n",
        "\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"image\"\n",
        "            self.data_dim = 2\n",
        "        elif data_type == \"sequence\":\n",
        "            self.data_type = data_type\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "        # Create transforms\n",
        "        train_transform = [\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "        # add augmentations\n",
        "        if kwargs[\"augment\"]:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.train_transform = transforms.Compose(train_transform)\n",
        "\n",
        "    def download_and_extract_lra_release(self, data_dir):\n",
        "        url = \"https://storage.googleapis.com/long-range-arena/lra_release.gz\"\n",
        "        local_filename = os.path.join(data_dir, \"lra_release.gz\")\n",
        "\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the tar.gz file\n",
        "        with tarfile.open(local_filename, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_dir)\n",
        "\n",
        "        # Optionally, remove the tar.gz file after extraction\n",
        "        os.remove(local_filename)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not self.data_dir.is_dir():\n",
        "\n",
        "            self.download_and_extract_lra_release(self.data_dir)\n",
        "            # raise FileNotFoundError(\n",
        "            #     f\"\"\"\n",
        "            # Directory {self.data_dir} not found.\n",
        "            # To get the dataset, download lra_release.gz from\n",
        "            # https://github.com/google-research/long-range-arena,\n",
        "            # then unzip it with tar -xvf lra_release.gz.\n",
        "            # Then point data_dir to the directory that contains pathfinderX, where X is the\n",
        "            # resolution (either 32, 64, 128, or 256).\n",
        "            # \"\"\"\n",
        "            # )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        # [2021-08-18] TD: I ran into RuntimeError: Too many open files.\n",
        "        # https://github.com/pytorch/pytorch/issues/11201\n",
        "        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
        "        dataset = PathFinderDataset(self.data_dir, transform=self.train_transform)\n",
        "        len_dataset = len(dataset)\n",
        "        val_len = int(self.val_split * len_dataset)\n",
        "        test_len = int(self.test_split * len_dataset)\n",
        "        train_len = len_dataset - val_len - test_len\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            dataset,\n",
        "            [train_len, val_len, test_len],\n",
        "            generator=torch.Generator().manual_seed(getattr(self, \"seed\", 42)),\n",
        "        )\n",
        "\n",
        "    # we define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "    def on_before_batch_transfer(self, batch, dataloader_idx):\n",
        "        if self.data_type == \"sequence\":\n",
        "            # If sequential, flatten the input [B, C, Y, X] -> [B, C, -1]\n",
        "            x, y = batch\n",
        "            x_shape = x.shape\n",
        "            # Flatten\n",
        "            x = x.view(x_shape[0], x_shape[1], -1)\n",
        "            batch = x, y\n",
        "        return batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yu2VOtclebU"
      },
      "source": [
        "# Path finder data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "mxLqmmpukzlx"
      },
      "outputs": [],
      "source": [
        "class PathfinderDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Pathfinder dataset created from a list of images.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, transform: Optional[Callable] = None) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_list (List[Tuple[str, int]]): List of tuples where each tuple contains\n",
        "                an image path and its corresponding label.\n",
        "            transform (Optional[Callable]): Optional transformation function or composition of transformations.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.img_list = self.create_imagelist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, int]: A tuple where the first element is the image tensor\n",
        "                and the second element is the label.\n",
        "        \"\"\"\n",
        "        img_path, label = self.img_list[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def create_imagelist(self) -> List[Tuple[str, int]]:\n",
        "\n",
        "        # root dir where the image are placed\n",
        "        directory = Path(self.data_dir).expanduser()\n",
        "\n",
        "        # metadata path where we get the class_idx\n",
        "        path_list = sorted(\n",
        "            list((directory / \"metadata\").glob(\"*.npy\")),\n",
        "            key=lambda path: int(path.stem),\n",
        "        )\n",
        "        instances = []\n",
        "        for metadata_file in path_list:\n",
        "            with open(metadata_file, \"r\") as f:\n",
        "                for metadata in f.read().splitlines():\n",
        "                    metadata = metadata.split()\n",
        "                    image_path = Path(metadata[0]) / metadata[1]\n",
        "                    label = int(metadata[3])\n",
        "                    instances.append((str(directory / image_path), label))\n",
        "        return instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPVgmR04lbYB"
      },
      "source": [
        "# Pathfinder data module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DB5lpStLlVnU"
      },
      "outputs": [],
      "source": [
        "class PathfinderDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg,\n",
        "        data_dir,\n",
        "        batch_size: int = 32,\n",
        "        test_batch_size: int = 32,\n",
        "        data_type=\"default\",\n",
        "        resolution = \"32\",\n",
        "        level=\"hard\",\n",
        "        val_split=0.1,\n",
        "        test_split=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        level_dir = {\n",
        "            \"easy\": \"curv_baseline\",\n",
        "            \"intermediate\": \"curv_contour_length_9\",\n",
        "            \"hard\": \"curv_contour_length_14\",\n",
        "        }[level]\n",
        "\n",
        "        # Save parameters to self\n",
        "        data_dir = (\n",
        "            data_dir + f\"/lra_release/pathfinder{resolution}/{level_dir}\"\n",
        "        )\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "\n",
        "        self.resolution = resolution\n",
        "        self.level = level\n",
        "\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        self.num_workers = 0  # for google colab training\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"image\"\n",
        "            self.data_dim = 2\n",
        "        elif data_type == \"sequence\":\n",
        "            self.data_type = data_type\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not self.data_dir.is_dir():\n",
        "            self.download_and_extract_lra_release(self.data_dir)\n",
        "\n",
        "    def download_and_extract_lra_release(self, data_dir):\n",
        "        url = \"https://storage.googleapis.com/long-range-arena/lra_release.gz\"\n",
        "        local_filename = os.path.join(data_dir, \"lra_release.gz\")\n",
        "\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the tar.gz file\n",
        "        with tarfile.open(local_filename, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_dir)\n",
        "\n",
        "        # Optionally, remove the tar.gz file after extraction\n",
        "        os.remove(local_filename)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self._set_transform()\n",
        "        #self._yaml_parameters()  # TODO set correct params\n",
        "\n",
        "        self.dataset = PathfinderDataset(self.data_dir, transform=self.transform)\n",
        "        # compute lengths\n",
        "\n",
        "        len_dataset = len(self.dataset)\n",
        "        val_len = int(self.val_split * len_dataset)\n",
        "        test_len = int(self.test_split * len_dataset)\n",
        "        train_len = len_dataset - val_len - test_len\n",
        "\n",
        "        # splits\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            self.dataset,\n",
        "            [train_len, val_len, test_len],\n",
        "            generator=torch.Generator().manual_seed(getattr(self, \"seed\", 42)),\n",
        "        )\n",
        "\n",
        "    def _set_transform(self):\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "      pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "    def on_before_batch_transfer(self, batch, dataloader_idx):\n",
        "        if self.data_type == \"sequence\":\n",
        "            x, y = batch\n",
        "            x_shape = x.shape\n",
        "            x = x.view(x_shape[0], x_shape[1], -1)\n",
        "            batch = x, y\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q62VcbqIrupw"
      },
      "source": [
        "# CCCN Implementation ListOps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KsGntQzGr4XH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ListOpsDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        pin_memory,\n",
        "        num_workers,\n",
        "        # Default values taken from S4\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dir = Path(data_dir) / \"lra_release/pathfinder32/curv_contour_length_14/lra_release/listops-1000\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if self.get_cache_dir() is None:\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                split_path = str(self.data_dir / f\"basic_{split}.tsv\")\n",
        "                print(split_path)\n",
        "        else:  # Process the dataset and save it\n",
        "            self.process_dataset()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        dataset, self.tokenizer = self.process_dataset()\n",
        "        self.vocab_size = len(self.tokenizer)\n",
        "\n",
        "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Target\"])\n",
        "\n",
        "        # Create all splits\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = (\n",
        "            dataset[\"train\"],\n",
        "            dataset[\"val\"],\n",
        "            dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            xs, ys = zip(*[(data[\"input_ids\"], data[\"Target\"]) for data in batch])\n",
        "            xs = torch.stack(\n",
        "                [\n",
        "                    torch.nn.functional.pad(\n",
        "                        x,\n",
        "                        [self.max_length - x.shape[-1], 0],\n",
        "                        value=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "                    for x in xs\n",
        "                ]\n",
        "            )\n",
        "            xs = xs.unsqueeze(1).float()\n",
        "            ys = torch.tensor(ys)\n",
        "            return xs, ys\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def process_dataset(self):\n",
        "        if self.get_cache_dir() is not None:\n",
        "            return self._load_from_cache()\n",
        "\n",
        "        dataset = load_dataset(\n",
        "            \"csv\",\n",
        "            data_files={\n",
        "                \"train\": str(self.data_dir / \"basic_train.tsv\"),\n",
        "                \"val\": str(self.data_dir / \"basic_val.tsv\"),\n",
        "                \"test\": str(self.data_dir / \"basic_test.tsv\"),\n",
        "            },\n",
        "            delimiter=\"\\t\",\n",
        "            keep_in_memory=True,\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "        # Adjust tokenizer for BOS and EOS tokens if needed\n",
        "        if self.append_bos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>']})\n",
        "        if self.append_eos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<eos>']})\n",
        "\n",
        "        tokenize = lambda example: {\n",
        "            \"tokens\": self.tokenizer(\n",
        "                example[\"Source\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "        }\n",
        "        dataset = dataset.map(\n",
        "            tokenize,\n",
        "            remove_columns=[\"Source\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        def numericalize(example):\n",
        "            tokens = (\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<bos>']) if self.append_bos else []) +\n",
        "                example[\"tokens\"] +\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<eos>']) if self.append_eos else [])\n",
        "            )\n",
        "            return {\"input_ids\": tokens}\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            numericalize,\n",
        "            remove_columns=[\"tokens\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self._save_to_cache(dataset)\n",
        "        return dataset, self.tokenizer\n",
        "\n",
        "    def _save_to_cache(self, dataset):\n",
        "        cache_dir = self.get_cache_dir()\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Saving to cache at {cache_dir}\")\n",
        "        dataset.save_to_disk(cache_dir)\n",
        "        with open(Path(cache_dir) / \"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.tokenizer, f)\n",
        "\n",
        "    def _load_from_cache(self):\n",
        "        assert self.get_cache_dir().is_dir()\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Load from cache at {self.get_cache_dir()}\")\n",
        "        dataset = load_dataset(self.get_cache_dir())\n",
        "        with open(Path(self.get_cache_dir()) / \"tokenizer.pkl\", \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        return dataset, tokenizer\n",
        "\n",
        "    @property\n",
        "    def _cache_dir_name(self):\n",
        "        return f\"max_length-{self.max_length}-append_bos-{self.append_bos}-append_eos-{self.append_eos}\"\n",
        "\n",
        "    def get_cache_dir(self):\n",
        "        cache_dir = self.data_dir / self._cache_dir_name\n",
        "        return cache_dir if cache_dir.is_dir() else None\n",
        "\n",
        "    # We define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbGB5GQLgNZo"
      },
      "source": [
        "# ListOps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3DWJLRe4gK7A"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional, List, Tuple\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "class ListOpsDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dir = Path(data_dir) / \"lra_release/pathfinder32/curv_contour_length_14/lra_release/listops-1000\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = 7\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "\n",
        "        dataset = load_dataset(\n",
        "            \"csv\",\n",
        "            data_files={\n",
        "                \"train\": str(self.data_dir / \"basic_train.tsv\"),\n",
        "                \"val\": str(self.data_dir / \"basic_val.tsv\"),\n",
        "                \"test\": str(self.data_dir / \"basic_test.tsv\"),\n",
        "            },\n",
        "            delimiter=\"\\t\",\n",
        "            keep_in_memory=True,\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "        # Adjust tokenizer for BOS and EOS tokens if needed\n",
        "        if self.append_bos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>']})\n",
        "        if self.append_eos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<eos>']})\n",
        "\n",
        "        tokenize = lambda example: {\n",
        "            \"tokens\": self.tokenizer(\n",
        "                example[\"Source\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "        }\n",
        "        dataset = dataset.map(\n",
        "            tokenize,\n",
        "            remove_columns=[\"Source\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        def numericalize(example):\n",
        "            tokens = (\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<bos>']) if self.append_bos else []) +\n",
        "                example[\"tokens\"] +\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<eos>']) if self.append_eos else [])\n",
        "            )\n",
        "            return {\"input_ids\": tokens}\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            numericalize,\n",
        "            remove_columns=[\"tokens\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self._set_transform()\n",
        "        self._yaml_parameters()  # TODO set correct params\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Target\"])\n",
        "\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = (\n",
        "            self.dataset[\"train\"],\n",
        "            self.dataset[\"val\"],\n",
        "            self.dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            input_ids = [data[\"input_ids\"] for data in batch]\n",
        "            labels = [data[\"Target\"] for data in batch]\n",
        "\n",
        "            pad_value = self.vocab.pad_index\n",
        "\n",
        "            padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "                [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
        "            )\n",
        "\n",
        "            if padded_input_ids.size(1) > self.max_length:\n",
        "                padded_input_ids = padded_input_ids[:, -self.max_length:]  # truncate to max_length\n",
        "            else:\n",
        "                padding_size = self.max_length - padded_input_ids.size(1)\n",
        "                padded_input_ids = torch.nn.functional.pad(\n",
        "                    padded_input_ids,\n",
        "                    (padding_size, 0),  # pad on the left\n",
        "                    value=pad_value,\n",
        "                )\n",
        "\n",
        "            input_tensor = padded_input_ids.float()\n",
        "            label_tensor = torch.tensor(labels)\n",
        "\n",
        "            return input_tensor, label_tensor\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def _set_transform(self):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "      pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHpAy4cJry9B"
      },
      "source": [
        "# CCCN Implementation IMDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OsDU9nKTr6f1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        pin_memory,\n",
        "        num_workers,\n",
        "        # Default values taken from S4\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        vocab_min_freq=15,\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        val_split=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save parameters to self\n",
        "        self.data_dir = Path(data_dir) / \"IMDB\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.vocab_min_freq = vocab_min_freq\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "        self.val_split = val_split\n",
        "\n",
        "        self.tokenizer = None\n",
        "        self.cache_dir = self.get_cache_dir()\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if self.cache_dir is None:  # Just download the dataset\n",
        "            load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "            self.process_dataset()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        dataset, self.tokenizer = self.process_dataset()\n",
        "        self.vocab_size = len(self.tokenizer)\n",
        "\n",
        "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "        # Create all splits\n",
        "        self.train_dataset, self.test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
        "        if self.val_split == 0.0:\n",
        "            # Use test set as val set, as done in the LRA paper\n",
        "            self.val_dataset = self.test_dataset\n",
        "        else:\n",
        "            train_val = self.train_dataset.train_test_split(\n",
        "                test_size=self.val_split,\n",
        "                seed=getattr(self, \"seed\", 42),\n",
        "            )\n",
        "            self.train_dataset, self.val_dataset = train_val[\"train\"], train_val[\"test\"]\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            xs, ys = zip(*[(data[\"input_ids\"], data[\"label\"]) for data in batch])\n",
        "            xs = torch.stack(\n",
        "                [\n",
        "                    torch.nn.functional.pad(\n",
        "                        x,\n",
        "                        [self.max_length - len(x), 0],\n",
        "                        value=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "                    for x in xs\n",
        "                ]\n",
        "            )\n",
        "            xs = xs.unsqueeze(1).float()\n",
        "            ys = torch.tensor(ys)\n",
        "            return xs, ys\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def process_dataset(self):\n",
        "        if self.get_cache_dir() is not None:\n",
        "            return self._load_from_cache()\n",
        "\n",
        "        dataset = load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>', '<eos>']})\n",
        "\n",
        "        def tokenize_function(example):\n",
        "            encoding = self.tokenizer(\n",
        "                example[\"text\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": encoding['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "            }\n",
        "\n",
        "        # Tokenize and map to dataset\n",
        "        tokenized_datasets = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"text\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self._save_to_cache(tokenized_datasets)\n",
        "        return tokenized_datasets, self.tokenizer\n",
        "\n",
        "    def _save_to_cache(self, dataset):\n",
        "        cache_dir = self.get_cache_dir()\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Saving to cache at {cache_dir}\")\n",
        "        dataset.save_to_disk(cache_dir)\n",
        "        with open(Path(cache_dir) / \"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.tokenizer, f)\n",
        "\n",
        "    def _load_from_cache(self):\n",
        "        assert self.get_cache_dir().is_dir()\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Load from cache at {self.get_cache_dir()}\")\n",
        "        dataset = DatasetDict.load_from_disk(self.get_cache_dir())\n",
        "        with open(Path(self.get_cache_dir()) / \"tokenizer.pkl\", \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        return dataset, tokenizer\n",
        "\n",
        "    @property\n",
        "    def _cache_dir_name(self):\n",
        "        return f\"l_max-{self.max_length}-tokenizer-{self.tokenizer_name}-min_freq-{self.vocab_min_freq}-append_bos-{self.append_bos}-append_eos-{self.append_eos}\"\n",
        "\n",
        "    def get_cache_dir(self):\n",
        "        cache_dir = self.data_dir / self._cache_dir_name\n",
        "        return cache_dir if cache_dir.is_dir() else None\n",
        "\n",
        "    # We define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqJb48C3gR2Q"
      },
      "source": [
        "# IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FmJe6kZpgUaU"
      },
      "outputs": [],
      "source": [
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        max_length=4096,\n",
        "        tokenizer_type=\"word\",\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        vocab_min_freq=15,\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        val_split=0.0,\n",
        "    ):\n",
        "        assert tokenizer_type in [\n",
        "            \"word\",\n",
        "            \"char\",\n",
        "        ], f\"tokenizer_type {tokenizer_type} not supported\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Save parameters to self\n",
        "        self.data_dir = Path(data_dir) / \"IMDB\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = 7\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer_type = tokenizer_type\n",
        "        self.vocab_min_freq = vocab_min_freq\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "        self.val_split = val_split\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "\n",
        "        dataset = load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>', '<eos>']})\n",
        "\n",
        "        def tokenize_function(example):\n",
        "            encoding = self.tokenizer(\n",
        "                example[\"text\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": encoding['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "            }\n",
        "\n",
        "        # Tokenize and map to dataset\n",
        "        tokenized_datasets = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"text\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self.dataset = tokenized_datasets\n",
        "\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        self._set_transform()\n",
        "        self._yaml_parameters()\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "        self.train_dataset, self.test_dataset = (\n",
        "            self.dataset[\"train\"],\n",
        "            self.dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            input_ids = [data[\"input_ids\"] for data in batch]\n",
        "            labels = [data[\"label\"] for data in batch]\n",
        "\n",
        "            pad_value = float(self.vocab[\"<pad>\"])\n",
        "\n",
        "            padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "                input_ids, batch_first=True, padding_value=pad_value\n",
        "            )\n",
        "\n",
        "            if padded_input_ids.size(1) > self.max_length:\n",
        "                padded_input_ids = padded_input_ids[\n",
        "                    :, -self.max_length :\n",
        "                ]  # truncate to max_length\n",
        "            else:\n",
        "                # Pad to max_length on the left (if needed)\n",
        "                padding_size = self.max_length - padded_input_ids.size(1)\n",
        "                padded_input_ids = torch.nn.functional.pad(\n",
        "                    padded_input_ids,\n",
        "                    (padding_size, 0),  # pad on the left\n",
        "                    value=pad_value,\n",
        "                )\n",
        "\n",
        "            input_tensor = padded_input_ids.unsqueeze(1).float()\n",
        "\n",
        "            label_tensor = torch.tensor(labels)\n",
        "\n",
        "            return input_tensor, label_tensor\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def _set_transform(self):\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wg7I6c-ZO90"
      },
      "source": [
        "# Main code run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856,
          "referenced_widgets": [
            "51e38a91f38e4814b8a84e7ae41ac923",
            "b75d71dc9a2345c58d76d23064d53183",
            "1404423674c54b77b5519bd75c0555bd",
            "945a0014f0c8466d8d4dc78153efbff0",
            "03004cb66ef94f638c2637f858943aec",
            "6be8c65e4fe2446da384ac79b3d68e50",
            "bfeb59ee9c914a1199dc1b256dd7178b",
            "9c6945fa638f4781a56f313a2d83ca4a",
            "f33069a4b52a46b9ad8cf29d9a2e37f0",
            "1b2fe6bfaf9641208c4b6aac012a7229",
            "651915f5de9b4f10a75683cd28ce0018",
            "1aaa1dddce9e475d8b93594fcaf457a3",
            "b6f6e2b23232457a9cc01d144458bfa9",
            "62b11892f8e44cde9957296bfb703609",
            "3589ef4bf2744527942c0a358e4df269",
            "aa3521f4dbd540dc8e490cdb10d77682",
            "ba12ab8c99784a7296ae337e0e718cc4",
            "9a15e03b50204af0a5d2fc5ef999e540",
            "d78dad922536445ebbba5ffb8a1d7f19",
            "b6a32994c4a64aca840589de430cac74",
            "32018223a5544af4b35a94ba1fb578f4",
            "3c8034f98977410f9dc4be2da1529bc2"
          ]
        },
        "id": "Js7mYkgQ8pG_",
        "outputId": "b305213a-f6dd-4dc9-c89c-655a34f4f6f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51e38a91f38e4814b8a84e7ae41ac923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=7):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aaa1dddce9e475d8b93594fcaf457a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=7):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-15:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n",
            "    for i, result in enumerate(func(**kwargs)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3581, in _map_single\n",
            "    writer.write_batch(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 569, in write_batch\n",
            "    inferred_features[col] = typed_sequence.get_inferred_type()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 133, in get_inferred_type\n",
            "    self._inferred_type = generate_from_arrow_type(pa.array(self).type)\n",
            "  File \"pyarrow/array.pxi\", line 248, in pyarrow.lib.array\n",
            "  File \"pyarrow/array.pxi\", line 112, in pyarrow.lib._handle_arrow_array_protocol\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 193, in __arrow_array__\n",
            "    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "TimeoutError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/managers.py\u001b[0m in \u001b[0;36m_callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-850673660d8a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"imdb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDBDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Retrieve and print a sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-bbf23bfc0c12>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Tokenize and map to dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         tokenized_datasets = dataset.map(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 869\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    870\u001b[0m                 k: dataset.map(\n\u001b[1;32m    871\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    868\u001b[0m         return DatasetDict(\n\u001b[1;32m    869\u001b[0m             {\n\u001b[0;32m--> 870\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    871\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         }\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3257\u001b[0m                         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" (num_proc={num_proc})\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m                     ) as pbar:\n\u001b[0;32m-> 3259\u001b[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001b[0m\u001b[1;32m   3260\u001b[0m                             \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_per_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m                         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# prompt: Generate the code to instantiate PathFinderDataModule\n",
        "\n",
        "choice = \"imdb\"\n",
        "if choice == \"path_finder\":\n",
        "  dm = PathFinderDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default', num_workers=2, pin_memory=False, resolution=32,augment = False)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "\n",
        "\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # prompt: Generate the code to instantiate PathFinderDataModule\n",
        "  dm = PathfinderDataModule(data_dir='./', batch_size=32, test_batch_size=32)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Fetch a sample from the training dataset\n",
        "  sample_idx = 0  # Index of the sample you want to print\n",
        "  sample = dm.train_dataset[sample_idx]\n",
        "\n",
        "  # If the sample is a tuple (image, label)\n",
        "  if isinstance(sample, tuple):\n",
        "      image, label = sample\n",
        "  else:\n",
        "      image = sample\n",
        "      label = None\n",
        "\n",
        "  # Print label\n",
        "  if label is not None:\n",
        "      print(f\"Label: {label}\")\n",
        "\n",
        "  # Print image\n",
        "  # Assuming image is a PIL image or a tensor that can be converted to PIL\n",
        "  if isinstance(image, torch.Tensor):\n",
        "      image = transforms.ToPILImage()(image)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"Sample {sample_idx}\")\n",
        "  plt.axis('off')  # Hide axis\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "elif choice == \"listops\":\n",
        "  dm = ListOpsDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default')\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Retrieve and print a sample\n",
        "  train_loader = dm.train_dataloader()\n",
        "\n",
        "  # Get a batch of data\n",
        "  for images, labels in train_loader:\n",
        "      print(f\"Batch of images shape: {images.shape}\")\n",
        "      print(f\"Batch of labels: {labels}\")\n",
        "\n",
        "      # Print the first image and label in the batch\n",
        "      print(f\"First image tensor: {images[0]}\")\n",
        "      print(f\"First label: {labels[0]}\")\n",
        "\n",
        "      # Break after first batch for demonstration\n",
        "      break\n",
        "\n",
        "elif choice == \"imdb\":\n",
        "  dm = IMDBDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default')\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Retrieve and print a sample\n",
        "  train_loader = dm.train_dataloader()\n",
        "\n",
        "  # Get a batch of data\n",
        "  for images, labels in train_loader:\n",
        "      print(f\"Batch of images shape: {images.shape}\")\n",
        "      print(f\"Batch of labels: {labels}\")\n",
        "\n",
        "      # Print the first image and label in the batch\n",
        "      print(f\"First image tensor: {images[0]}\")\n",
        "      print(f\"First label: {labels[0]}\")\n",
        "\n",
        "      # Break after first batch for demonstration\n",
        "      break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9NfQnd5BX9eg",
        "MhxM4KubZfhc",
        "4Yu2VOtclebU",
        "NPVgmR04lbYB",
        "Q62VcbqIrupw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03004cb66ef94f638c2637f858943aec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1404423674c54b77b5519bd75c0555bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c6945fa638f4781a56f313a2d83ca4a",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f33069a4b52a46b9ad8cf29d9a2e37f0",
            "value": 25000
          }
        },
        "1aaa1dddce9e475d8b93594fcaf457a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6f6e2b23232457a9cc01d144458bfa9",
              "IPY_MODEL_62b11892f8e44cde9957296bfb703609",
              "IPY_MODEL_3589ef4bf2744527942c0a358e4df269"
            ],
            "layout": "IPY_MODEL_aa3521f4dbd540dc8e490cdb10d77682"
          }
        },
        "1b2fe6bfaf9641208c4b6aac012a7229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32018223a5544af4b35a94ba1fb578f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3589ef4bf2744527942c0a358e4df269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32018223a5544af4b35a94ba1fb578f4",
            "placeholder": "",
            "style": "IPY_MODEL_3c8034f98977410f9dc4be2da1529bc2",
            "value": "8000/25000[01:11&lt;02:57,96.03examples/s]"
          }
        },
        "3c8034f98977410f9dc4be2da1529bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51e38a91f38e4814b8a84e7ae41ac923": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b75d71dc9a2345c58d76d23064d53183",
              "IPY_MODEL_1404423674c54b77b5519bd75c0555bd",
              "IPY_MODEL_945a0014f0c8466d8d4dc78153efbff0"
            ],
            "layout": "IPY_MODEL_03004cb66ef94f638c2637f858943aec"
          }
        },
        "62b11892f8e44cde9957296bfb703609": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d78dad922536445ebbba5ffb8a1d7f19",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6a32994c4a64aca840589de430cac74",
            "value": 8000
          }
        },
        "651915f5de9b4f10a75683cd28ce0018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6be8c65e4fe2446da384ac79b3d68e50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945a0014f0c8466d8d4dc78153efbff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2fe6bfaf9641208c4b6aac012a7229",
            "placeholder": "",
            "style": "IPY_MODEL_651915f5de9b4f10a75683cd28ce0018",
            "value": "25000/25000[02:30&lt;00:00,253.42examples/s]"
          }
        },
        "9a15e03b50204af0a5d2fc5ef999e540": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c6945fa638f4781a56f313a2d83ca4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa3521f4dbd540dc8e490cdb10d77682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a32994c4a64aca840589de430cac74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6f6e2b23232457a9cc01d144458bfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba12ab8c99784a7296ae337e0e718cc4",
            "placeholder": "",
            "style": "IPY_MODEL_9a15e03b50204af0a5d2fc5ef999e540",
            "value": "Map(num_proc=7):32%"
          }
        },
        "b75d71dc9a2345c58d76d23064d53183": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be8c65e4fe2446da384ac79b3d68e50",
            "placeholder": "",
            "style": "IPY_MODEL_bfeb59ee9c914a1199dc1b256dd7178b",
            "value": "Map(num_proc=7):100%"
          }
        },
        "ba12ab8c99784a7296ae337e0e718cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfeb59ee9c914a1199dc1b256dd7178b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d78dad922536445ebbba5ffb8a1d7f19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f33069a4b52a46b9ad8cf29d9a2e37f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
