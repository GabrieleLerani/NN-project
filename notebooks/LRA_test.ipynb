{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MhxM4KubZfhc",
        "4Yu2VOtclebU",
        "NPVgmR04lbYB",
        "Q62VcbqIrupw",
        "FHpAy4cJry9B"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "9NfQnd5BX9eg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XzWk0t-20n3Z",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d59d70d6-7302-4042-aafe-ac34e7dc86a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.11.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 pytorch_lightning-2.4.0 torchmetrics-1.4.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=ab10c05a619f67bad8fc438ea0beba6430664bbdff90755055d18cab93f61a48\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "1725ccd4885945fdbbded2ed16644211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install pytorch_lightning\n",
        "%pip install datasets\n",
        "%pip install hydra-core\n",
        "%pip install -U portalocker>=2.0.0\n",
        "#%pip install -r requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Imports"
      ],
      "metadata": {
        "id": "GAIl2ZyHYAhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from typing import Optional, Callable, Tuple, Dict, List, cast\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "import requests\n",
        "\n",
        "from hydra import utils\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import logging\n",
        "import pickle\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#import torchtext"
      ],
      "metadata": {
        "id": "DCZf8pQWX8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CCCN Implementation Path Finder\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MhxM4KubZfhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There's an empty file in the dataset\n",
        "PATHFINDER_BLACKLIST = {\"pathfinder32/curv_baseline/imgs/0/sample_172.png\"}\n",
        "\n",
        "\n",
        "def pil_loader_grayscale(path: str) -> Image.Image:\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return Image.open(f).convert(\"L\")\n",
        "\n",
        "\n",
        "class PathFinderDataset(datasets.ImageFolder):\n",
        "    \"\"\"Path Finder dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            root,\n",
        "            loader=pil_loader_grayscale,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform,\n",
        "            is_valid_file=is_valid_file,\n",
        "        )\n",
        "\n",
        "    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "        \"\"\"Override this so it doesn't call the parent's method.\"\"\"\n",
        "        return [], {}\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataset(\n",
        "        directory: str,\n",
        "        class_to_idx: Dict[str, int],\n",
        "        extensions: Optional[Tuple[str, ...]] = None,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "        allow_empty: bool = False\n",
        "    ) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\"\"\"\n",
        "        directory = Path(directory).expanduser()\n",
        "\n",
        "        both_none = extensions is None and is_valid_file is None\n",
        "        both_something = extensions is not None and is_valid_file is not None\n",
        "        if both_none or both_something:\n",
        "            raise ValueError(\n",
        "                \"Both extensions and is_valid_file cannot be None or not None at the same time\"\n",
        "            )\n",
        "\n",
        "        if extensions is not None:\n",
        "            def is_valid_file(x: str) -> bool:\n",
        "                return datasets.folder.has_file_allowed_extension(\n",
        "                    x, cast(Tuple[str, ...], extensions)\n",
        "                )\n",
        "        is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
        "\n",
        "        path_list = sorted(\n",
        "            list((directory / \"metadata\").glob(\"*.npy\")),\n",
        "            key=lambda path: int(path.stem),\n",
        "        )\n",
        "        if not path_list:\n",
        "            raise FileNotFoundError(f\"No metadata found at {str(directory)}\")\n",
        "        # Get the 'pathfinder32/curv_baseline' part of data_dir\n",
        "        data_dir_stem = Path().joinpath(*directory.parts[-2:])\n",
        "        instances = []\n",
        "        for metadata_file in path_list:\n",
        "            with open(metadata_file, \"r\") as f:\n",
        "                for metadata in f.read().splitlines():\n",
        "                    metadata = metadata.split()\n",
        "                    image_path = Path(metadata[0]) / metadata[1]\n",
        "                    if (\n",
        "                        is_valid_file(str(image_path))\n",
        "                        and str(data_dir_stem / image_path) not in PATHFINDER_BLACKLIST\n",
        "                    ):\n",
        "                        label = int(metadata[3])\n",
        "                        instances.append((str(directory / image_path), label))\n",
        "        return instances\n",
        "\n",
        "\n",
        "\n",
        "class PathFinderDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        num_workers,\n",
        "        pin_memory,\n",
        "        resolution,\n",
        "        level=\"hard\",\n",
        "        val_split=0.1,\n",
        "        test_split=0.1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert resolution in [32, 64, 128, 256]\n",
        "        assert level in [\"easy\", \"intermediate\", \"hard\"]\n",
        "\n",
        "        level_dir = {\n",
        "            \"easy\": \"curv_baseline\",\n",
        "            \"intermediate\": \"curv_contour_length_9\",\n",
        "            \"hard\": \"curv_contour_length_14\",\n",
        "        }[level]\n",
        "\n",
        "        # Save parameters to self\n",
        "        data_dir = (\n",
        "            data_dir + f\"/lra_release/pathfinder32/curv_contour_length_14/lra_release/lra_release/pathfinder{resolution}/{level_dir}\"\n",
        "        )\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.resolution = resolution\n",
        "        self.level = level\n",
        "\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"image\"\n",
        "            self.data_dim = 2\n",
        "        elif data_type == \"sequence\":\n",
        "            self.data_type = data_type\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "        # Create transforms\n",
        "        train_transform = [\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "        # add augmentations\n",
        "        if kwargs[\"augment\"]:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.train_transform = transforms.Compose(train_transform)\n",
        "\n",
        "    def download_and_extract_lra_release(self, data_dir):\n",
        "        url = \"https://storage.googleapis.com/long-range-arena/lra_release.gz\"\n",
        "        local_filename = os.path.join(data_dir, \"lra_release.gz\")\n",
        "\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the tar.gz file\n",
        "        with tarfile.open(local_filename, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_dir)\n",
        "\n",
        "        # Optionally, remove the tar.gz file after extraction\n",
        "        os.remove(local_filename)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not self.data_dir.is_dir():\n",
        "\n",
        "            self.download_and_extract_lra_release(self.data_dir)\n",
        "            # raise FileNotFoundError(\n",
        "            #     f\"\"\"\n",
        "            # Directory {self.data_dir} not found.\n",
        "            # To get the dataset, download lra_release.gz from\n",
        "            # https://github.com/google-research/long-range-arena,\n",
        "            # then unzip it with tar -xvf lra_release.gz.\n",
        "            # Then point data_dir to the directory that contains pathfinderX, where X is the\n",
        "            # resolution (either 32, 64, 128, or 256).\n",
        "            # \"\"\"\n",
        "            # )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        # [2021-08-18] TD: I ran into RuntimeError: Too many open files.\n",
        "        # https://github.com/pytorch/pytorch/issues/11201\n",
        "        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
        "        dataset = PathFinderDataset(self.data_dir, transform=self.train_transform)\n",
        "        len_dataset = len(dataset)\n",
        "        val_len = int(self.val_split * len_dataset)\n",
        "        test_len = int(self.test_split * len_dataset)\n",
        "        train_len = len_dataset - val_len - test_len\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            dataset,\n",
        "            [train_len, val_len, test_len],\n",
        "            generator=torch.Generator().manual_seed(getattr(self, \"seed\", 42)),\n",
        "        )\n",
        "\n",
        "    # we define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "    def on_before_batch_transfer(self, batch, dataloader_idx):\n",
        "        if self.data_type == \"sequence\":\n",
        "            # If sequential, flatten the input [B, C, Y, X] -> [B, C, -1]\n",
        "            x, y = batch\n",
        "            x_shape = x.shape\n",
        "            # Flatten\n",
        "            x = x.view(x_shape[0], x_shape[1], -1)\n",
        "            batch = x, y\n",
        "        return batch\n",
        "\n"
      ],
      "metadata": {
        "id": "-ycdt1gJ00V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path finder data set"
      ],
      "metadata": {
        "id": "4Yu2VOtclebU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PathfinderDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Pathfinder dataset created from a list of images.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, transform: Optional[Callable] = None) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_list (List[Tuple[str, int]]): List of tuples where each tuple contains\n",
        "                an image path and its corresponding label.\n",
        "            transform (Optional[Callable]): Optional transformation function or composition of transformations.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.img_list = self.create_imagelist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, int]: A tuple where the first element is the image tensor\n",
        "                and the second element is the label.\n",
        "        \"\"\"\n",
        "        img_path, label = self.img_list[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def create_imagelist(self) -> List[Tuple[str, int]]:\n",
        "\n",
        "        # root dir where the image are placed\n",
        "        directory = Path(self.data_dir).expanduser()\n",
        "\n",
        "        # metadata path where we get the class_idx\n",
        "        path_list = sorted(\n",
        "            list((directory / \"metadata\").glob(\"*.npy\")),\n",
        "            key=lambda path: int(path.stem),\n",
        "        )\n",
        "        instances = []\n",
        "        for metadata_file in path_list:\n",
        "            with open(metadata_file, \"r\") as f:\n",
        "                for metadata in f.read().splitlines():\n",
        "                    metadata = metadata.split()\n",
        "                    image_path = Path(metadata[0]) / metadata[1]\n",
        "                    label = int(metadata[3])\n",
        "                    instances.append((str(directory / image_path), label))\n",
        "        return instances"
      ],
      "metadata": {
        "id": "mxLqmmpukzlx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathfinder data module"
      ],
      "metadata": {
        "id": "NPVgmR04lbYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PathfinderDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size: int = 32,\n",
        "        test_batch_size: int = 32,\n",
        "        data_type=\"default\",\n",
        "        resolution = \"32\",\n",
        "        level=\"hard\",\n",
        "        val_split=0.1,\n",
        "        test_split=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        level_dir = {\n",
        "            \"easy\": \"curv_baseline\",\n",
        "            \"intermediate\": \"curv_contour_length_9\",\n",
        "            \"hard\": \"curv_contour_length_14\",\n",
        "        }[level]\n",
        "\n",
        "        # Save parameters to self\n",
        "        data_dir = (\n",
        "            data_dir + f\"/lra_release/pathfinder{resolution}/{level_dir}\"\n",
        "        )\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "\n",
        "        self.resolution = resolution\n",
        "        self.level = level\n",
        "\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "\n",
        "        self.num_workers = 0  # for google colab training\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"image\"\n",
        "            self.data_dim = 2\n",
        "        elif data_type == \"sequence\":\n",
        "            self.data_type = data_type\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not self.data_dir.is_dir():\n",
        "            self.download_and_extract_lra_release(self.data_dir)\n",
        "\n",
        "    def download_and_extract_lra_release(self, data_dir):\n",
        "        if os.path.exists(Path(data_dir) / \"lra_release\"):\n",
        "            print(\n",
        "                f\"Directory {data_dir} already exists. Skipping download and extraction.\"\n",
        "            )\n",
        "            return\n",
        "        url = \"https://storage.googleapis.com/long-range-arena/lra_release.gz\"\n",
        "        local_filename = os.path.join(data_dir, \"lra_release.gz\")\n",
        "\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the tar.gz file\n",
        "        with tarfile.open(local_filename, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_dir)\n",
        "\n",
        "        # Optionally, remove the tar.gz file after extraction\n",
        "        os.remove(local_filename)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self._set_transform()\n",
        "        #self._yaml_parameters()  # TODO set correct params\n",
        "\n",
        "        self.dataset = PathfinderDataset(self.data_dir, transform=self.transform)\n",
        "        # compute lengths\n",
        "\n",
        "        len_dataset = len(self.dataset)\n",
        "        val_len = int(self.val_split * len_dataset)\n",
        "        test_len = int(self.test_split * len_dataset)\n",
        "        train_len = len_dataset - val_len - test_len\n",
        "\n",
        "        # splits\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            self.dataset,\n",
        "            [train_len, val_len, test_len],\n",
        "            generator=torch.Generator().manual_seed(getattr(self, \"seed\", 42)),\n",
        "        )\n",
        "\n",
        "    def _set_transform(self):\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "      pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "    def on_before_batch_transfer(self, batch, dataloader_idx):\n",
        "        if self.data_type == \"sequence\":\n",
        "            x, y = batch\n",
        "            x_shape = x.shape\n",
        "            x = x.view(x_shape[0], x_shape[1], -1)\n",
        "            batch = x, y\n",
        "        return batch"
      ],
      "metadata": {
        "id": "DB5lpStLlVnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CCCN Implementation ListOps\n"
      ],
      "metadata": {
        "id": "Q62VcbqIrupw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ListOpsDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        pin_memory,\n",
        "        num_workers,\n",
        "        # Default values taken from S4\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dir = Path(data_dir) / \"lra_release/pathfinder32/curv_contour_length_14/lra_release/listops-1000\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if self.get_cache_dir() is None:\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                split_path = str(self.data_dir / f\"basic_{split}.tsv\")\n",
        "                print(split_path)\n",
        "        else:  # Process the dataset and save it\n",
        "            self.process_dataset()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        dataset, self.tokenizer = self.process_dataset()\n",
        "        self.vocab_size = len(self.tokenizer)\n",
        "\n",
        "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Target\"])\n",
        "\n",
        "        # Create all splits\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = (\n",
        "            dataset[\"train\"],\n",
        "            dataset[\"val\"],\n",
        "            dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            xs, ys = zip(*[(data[\"input_ids\"], data[\"Target\"]) for data in batch])\n",
        "            xs = torch.stack(\n",
        "                [\n",
        "                    torch.nn.functional.pad(\n",
        "                        x,\n",
        "                        [self.max_length - x.shape[-1], 0],\n",
        "                        value=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "                    for x in xs\n",
        "                ]\n",
        "            )\n",
        "            xs = xs.unsqueeze(1).float()\n",
        "            ys = torch.tensor(ys)\n",
        "            return xs, ys\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def process_dataset(self):\n",
        "        if self.get_cache_dir() is not None:\n",
        "            return self._load_from_cache()\n",
        "\n",
        "        dataset = load_dataset(\n",
        "            \"csv\",\n",
        "            data_files={\n",
        "                \"train\": str(self.data_dir / \"basic_train.tsv\"),\n",
        "                \"val\": str(self.data_dir / \"basic_val.tsv\"),\n",
        "                \"test\": str(self.data_dir / \"basic_test.tsv\"),\n",
        "            },\n",
        "            delimiter=\"\\t\",\n",
        "            keep_in_memory=True,\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "        # Adjust tokenizer for BOS and EOS tokens if needed\n",
        "        if self.append_bos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>']})\n",
        "        if self.append_eos:\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<eos>']})\n",
        "\n",
        "        tokenize = lambda example: {\n",
        "            \"tokens\": self.tokenizer(\n",
        "                example[\"Source\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "        }\n",
        "        dataset = dataset.map(\n",
        "            tokenize,\n",
        "            remove_columns=[\"Source\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        def numericalize(example):\n",
        "            tokens = (\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<bos>']) if self.append_bos else []) +\n",
        "                example[\"tokens\"] +\n",
        "                (self.tokenizer.convert_tokens_to_ids(['<eos>']) if self.append_eos else [])\n",
        "            )\n",
        "            return {\"input_ids\": tokens}\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            numericalize,\n",
        "            remove_columns=[\"tokens\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self._save_to_cache(dataset)\n",
        "        return dataset, self.tokenizer\n",
        "\n",
        "    def _save_to_cache(self, dataset):\n",
        "        cache_dir = self.get_cache_dir()\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Saving to cache at {cache_dir}\")\n",
        "        dataset.save_to_disk(cache_dir)\n",
        "        with open(Path(cache_dir) / \"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.tokenizer, f)\n",
        "\n",
        "    def _load_from_cache(self):\n",
        "        assert self.get_cache_dir().is_dir()\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Load from cache at {self.get_cache_dir()}\")\n",
        "        dataset = load_dataset(self.get_cache_dir())\n",
        "        with open(Path(self.get_cache_dir()) / \"tokenizer.pkl\", \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        return dataset, tokenizer\n",
        "\n",
        "    @property\n",
        "    def _cache_dir_name(self):\n",
        "        return f\"max_length-{self.max_length}-append_bos-{self.append_bos}-append_eos-{self.append_eos}\"\n",
        "\n",
        "    def get_cache_dir(self):\n",
        "        cache_dir = self.data_dir / self._cache_dir_name\n",
        "        return cache_dir if cache_dir.is_dir() else None\n",
        "\n",
        "    # We define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "KsGntQzGr4XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ListOps"
      ],
      "metadata": {
        "id": "jbGB5GQLgNZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional, List, Tuple\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from datasets import load_dataset,DatasetDict\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "\n",
        "class ListOpsDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dir = Path(data_dir) / \"datasets\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = 7\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 10\n",
        "\n",
        "    def prepare_data(self):\n",
        "\n",
        "        if not self.data_dir.is_dir():\n",
        "            self.download_and_extract_lra_release(self.data_dir)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "\n",
        "        serialized_dataset_path = os.path.join(self.data_dir, \"tokenized_dataset\")\n",
        "\n",
        "        if os.path.exists(serialized_dataset_path):\n",
        "            print(f\"Loading dataset from {serialized_dataset_path}...\")\n",
        "            self.dataset = DatasetDict.load_from_disk(serialized_dataset_path)\n",
        "        else:\n",
        "\n",
        "          dataset = load_dataset(\n",
        "              \"csv\",\n",
        "              data_files={\n",
        "                  \"train\": str(self.data_dir / \"lra_release/listops-1000/basic_train.tsv\"),\n",
        "                  \"val\": str(self.data_dir / \"lra_release/listops-1000/basic_val.tsv\"),\n",
        "                  \"test\": str(self.data_dir / \"lra_release/listops-1000/basic_test.tsv\"),\n",
        "              },\n",
        "              delimiter=\"\\t\",\n",
        "              keep_in_memory=True,\n",
        "          )\n",
        "\n",
        "          self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "          # Adjust tokenizer for BOS and EOS tokens if needed\n",
        "          if self.append_bos:\n",
        "              self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>']})\n",
        "          if self.append_eos:\n",
        "              self.tokenizer.add_special_tokens({'additional_special_tokens': ['<eos>']})\n",
        "\n",
        "          tokenize = lambda example: {\n",
        "              \"tokens\": self.tokenizer(\n",
        "                  example[\"Source\"],\n",
        "                  truncation=True,\n",
        "                  padding='max_length',\n",
        "                  max_length=self.max_length,\n",
        "                  return_tensors='pt'\n",
        "              )['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "          }\n",
        "          dataset = dataset.map(\n",
        "              tokenize,\n",
        "              remove_columns=[\"Source\"],\n",
        "              keep_in_memory=True,\n",
        "              load_from_cache_file=False,\n",
        "              num_proc=self.num_workers,\n",
        "          )\n",
        "\n",
        "          def numericalize(example):\n",
        "              tokens = (\n",
        "                  (self.tokenizer.convert_tokens_to_ids(['<bos>']) if self.append_bos else []) +\n",
        "                  example[\"tokens\"] +\n",
        "                  (self.tokenizer.convert_tokens_to_ids(['<eos>']) if self.append_eos else [])\n",
        "              )\n",
        "              return {\"input_ids\": tokens}\n",
        "\n",
        "          dataset = dataset.map(\n",
        "              numericalize,\n",
        "              remove_columns=[\"tokens\"],\n",
        "              keep_in_memory=True,\n",
        "              load_from_cache_file=False,\n",
        "              num_proc=self.num_workers,\n",
        "          )\n",
        "\n",
        "          self.dataset = dataset\n",
        "\n",
        "          print(f\"Saving dataset to {serialized_dataset_path}...\")\n",
        "          self.dataset.save_to_disk(serialized_dataset_path)\n",
        "\n",
        "\n",
        "    def download_and_extract_lra_release(self, data_dir):\n",
        "\n",
        "        if os.path.exists(Path(data_dir) / \"lra_release\"):\n",
        "            print(\n",
        "                f\"Directory {data_dir} already exists. Skipping download and extraction.\"\n",
        "            )\n",
        "            return\n",
        "        url = \"https://storage.googleapis.com/long-range-arena/lra_release.gz\"\n",
        "        local_filename = os.path.join(data_dir, \"lra_release.gz\")\n",
        "\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        # Extract the tar.gz file\n",
        "        with tarfile.open(local_filename, \"r:gz\") as tar:\n",
        "            tar.extractall(path=data_dir)\n",
        "\n",
        "        # Optionally, remove the tar.gz file after extraction\n",
        "        os.remove(local_filename)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self._set_transform()\n",
        "        self._yaml_parameters()  # TODO set correct params\n",
        "\n",
        "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"Target\"])\n",
        "\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = (\n",
        "            self.dataset[\"train\"],\n",
        "            self.dataset[\"val\"],\n",
        "            self.dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        def collate_batch(batch):\n",
        "          input_ids = [data[\"input_ids\"] for data in batch]\n",
        "          labels = [data[\"Target\"] for data in batch]  # Ensure this matches your dataset column name\n",
        "\n",
        "          # Retrieve the padding token ID from the tokenizer\n",
        "          pad_value = self.tokenizer.pad_token_id\n",
        "\n",
        "          # Convert lists of input IDs to tensors and pad them\n",
        "          padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "              [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
        "          )\n",
        "\n",
        "          # Truncate or pad to max_length\n",
        "          if padded_input_ids.size(1) > self.max_length:\n",
        "              padded_input_ids = padded_input_ids[:, -self.max_length:]  # Truncate to max_length\n",
        "          else:\n",
        "              padding_size = self.max_length - padded_input_ids.size(1)\n",
        "              padded_input_ids = torch.nn.functional.pad(\n",
        "                  padded_input_ids,\n",
        "                  (0, padding_size),  # Pad on the right\n",
        "                  value=pad_value,\n",
        "              )\n",
        "\n",
        "          input_tensor = padded_input_ids.float()\n",
        "          label_tensor = torch.tensor(labels)\n",
        "\n",
        "          return input_tensor, label_tensor\n",
        "\n",
        "        # Set the collate function\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "\n",
        "    def _set_transform(self):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "      pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "3DWJLRe4gK7A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CCCN Implementation IMDB\n"
      ],
      "metadata": {
        "id": "FHpAy4cJry9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        pin_memory,\n",
        "        num_workers,\n",
        "        # Default values taken from S4\n",
        "        max_length=512,  # Ensure this matches the model's max length\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        vocab_min_freq=15,\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        val_split=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save parameters to self\n",
        "        self.data_dir = Path(data_dir) / \"IMDB\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.vocab_min_freq = vocab_min_freq\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "        self.val_split = val_split\n",
        "\n",
        "        self.tokenizer = None\n",
        "        self.cache_dir = self.get_cache_dir()\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if self.cache_dir is None:  # Just download the dataset\n",
        "            load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "            self.process_dataset()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
        "            return\n",
        "        dataset, self.tokenizer = self.process_dataset()\n",
        "        self.vocab_size = len(self.tokenizer)\n",
        "\n",
        "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "        # Create all splits\n",
        "        self.train_dataset, self.test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
        "        if self.val_split == 0.0:\n",
        "            # Use test set as val set, as done in the LRA paper\n",
        "            self.val_dataset = self.test_dataset\n",
        "        else:\n",
        "            train_val = self.train_dataset.train_test_split(\n",
        "                test_size=self.val_split,\n",
        "                seed=getattr(self, \"seed\", 42),\n",
        "            )\n",
        "            self.train_dataset, self.val_dataset = train_val[\"train\"], train_val[\"test\"]\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            xs, ys = zip(*[(data[\"input_ids\"], data[\"label\"]) for data in batch])\n",
        "            xs = torch.stack(\n",
        "                [\n",
        "                    torch.nn.functional.pad(\n",
        "                        x,\n",
        "                        [self.max_length - len(x), 0],\n",
        "                        value=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "                    for x in xs\n",
        "                ]\n",
        "            )\n",
        "            xs = xs.unsqueeze(1).float()\n",
        "            ys = torch.tensor(ys)\n",
        "            return xs, ys\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def process_dataset(self):\n",
        "        if self.get_cache_dir() is not None:\n",
        "            return self._load_from_cache()\n",
        "\n",
        "        dataset = load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>', '<eos>']})\n",
        "\n",
        "        def tokenize_function(example):\n",
        "            encoding = self.tokenizer(\n",
        "                example[\"text\"],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": encoding['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "            }\n",
        "\n",
        "        # Tokenize and map to dataset\n",
        "        tokenized_datasets = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"text\"],\n",
        "            keep_in_memory=True,\n",
        "            load_from_cache_file=False,\n",
        "            num_proc=self.num_workers,\n",
        "        )\n",
        "\n",
        "        self._save_to_cache(tokenized_datasets)\n",
        "        return tokenized_datasets, self.tokenizer\n",
        "\n",
        "    def _save_to_cache(self, dataset):\n",
        "        cache_dir = self.get_cache_dir()\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Saving to cache at {cache_dir}\")\n",
        "        dataset.save_to_disk(cache_dir)\n",
        "        with open(Path(cache_dir) / \"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.tokenizer, f)\n",
        "\n",
        "    def _load_from_cache(self):\n",
        "        assert self.get_cache_dir().is_dir()\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Load from cache at {self.get_cache_dir()}\")\n",
        "        dataset = DatasetDict.load_from_disk(self.get_cache_dir())\n",
        "        with open(Path(self.get_cache_dir()) / \"tokenizer.pkl\", \"rb\") as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        return dataset, tokenizer\n",
        "\n",
        "    @property\n",
        "    def _cache_dir_name(self):\n",
        "        return f\"l_max-{self.max_length}-tokenizer-{self.tokenizer_name}-min_freq-{self.vocab_min_freq}-append_bos-{self.append_bos}-append_eos-{self.append_eos}\"\n",
        "\n",
        "    def get_cache_dir(self):\n",
        "        cache_dir = self.data_dir / self._cache_dir_name\n",
        "        return cache_dir if cache_dir.is_dir() else None\n",
        "\n",
        "    # We define a separate DataLoader for each of train/val/test\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "OsDU9nKTr6f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB"
      ],
      "metadata": {
        "id": "OqJb48C3gR2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        batch_size,\n",
        "        test_batch_size,\n",
        "        data_type,\n",
        "        max_length=4096,\n",
        "        tokenizer_type=\"word\",\n",
        "        tokenizer_name=\"bert-base-uncased\",\n",
        "        vocab_min_freq=15,\n",
        "        append_bos=False,\n",
        "        append_eos=True,\n",
        "        val_split=0.0,\n",
        "    ):\n",
        "        assert tokenizer_type in [\n",
        "            \"word\",\n",
        "            \"char\",\n",
        "        ], f\"tokenizer_type {tokenizer_type} not supported\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Save parameters to self\n",
        "        self.data_dir = Path(data_dir) / \"IMDB\"\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_workers = 7\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer_type = tokenizer_type\n",
        "        self.vocab_min_freq = vocab_min_freq\n",
        "        self.append_bos = append_bos\n",
        "        self.append_eos = append_eos\n",
        "        self.val_split = val_split\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "\n",
        "        # Determine data_type\n",
        "        if data_type == \"default\":\n",
        "            self.data_type = \"sequence\"\n",
        "            self.data_dim = 1\n",
        "        else:\n",
        "            raise ValueError(f\"data_type {data_type} not supported.\")\n",
        "\n",
        "        # Determine sizes of dataset\n",
        "        self.input_channels = 1\n",
        "        self.output_channels = 2\n",
        "\n",
        "    def prepare_data(self):\n",
        "        serialized_dataset_path = os.path.join(self.data_dir, \"tokenized_dataset\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "\n",
        "\n",
        "        if os.path.exists(serialized_dataset_path):\n",
        "            print(f\"Loading dataset from {serialized_dataset_path}...\")\n",
        "            self.dataset = DatasetDict.load_from_disk(serialized_dataset_path)\n",
        "        else:\n",
        "            dataset = load_dataset(\"imdb\", cache_dir=self.data_dir)\n",
        "\n",
        "            # Initialize tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<bos>', '<eos>']})\n",
        "\n",
        "            def tokenize_function(example):\n",
        "                encoding = self.tokenizer(\n",
        "                    example[\"text\"],\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    max_length=self.max_length,\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "                return {\n",
        "                    \"input_ids\": encoding['input_ids'].squeeze().tolist()  # Convert tensor to list\n",
        "                }\n",
        "\n",
        "            # Tokenize and map to dataset\n",
        "            tokenized_datasets = dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                remove_columns=[\"text\"],\n",
        "                keep_in_memory=True,\n",
        "                load_from_cache_file=False,\n",
        "                num_proc=self.num_workers,\n",
        "            )\n",
        "\n",
        "            self.dataset = tokenized_datasets\n",
        "\n",
        "            print(f\"Saving dataset to {serialized_dataset_path}...\")\n",
        "            self.dataset.save_to_disk(serialized_dataset_path)\n",
        "\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        self._set_transform()\n",
        "        self._yaml_parameters()\n",
        "\n",
        "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "        self.train_dataset, self.test_dataset = (\n",
        "            self.dataset[\"train\"],\n",
        "            self.dataset[\"test\"],\n",
        "        )\n",
        "\n",
        "        # Use tokenizer padding token ID\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            input_ids = [data[\"input_ids\"] for data in batch]\n",
        "            labels = [data[\"label\"] for data in batch]\n",
        "\n",
        "            padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "                [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.pad_token_id\n",
        "            )\n",
        "\n",
        "            if padded_input_ids.size(1) > self.max_length:\n",
        "                padded_input_ids = padded_input_ids[\n",
        "                    :, -self.max_length :\n",
        "                ]  # truncate to max_length\n",
        "            else:\n",
        "                # Pad to max_length on the left (if needed)\n",
        "                padding_size = self.max_length - padded_input_ids.size(1)\n",
        "                padded_input_ids = torch.nn.functional.pad(\n",
        "                    padded_input_ids,\n",
        "                    (padding_size, 0),  # pad on the left\n",
        "                    value=self.pad_token_id,\n",
        "                )\n",
        "\n",
        "            input_tensor = padded_input_ids.unsqueeze(1).float()\n",
        "            label_tensor = torch.tensor(labels)\n",
        "\n",
        "            return input_tensor, label_tensor\n",
        "\n",
        "        self.collate_fn = collate_batch\n",
        "\n",
        "    def _set_transform(self):\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return val_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_dataloader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.test_batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "FmJe6kZpgUaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main code run"
      ],
      "metadata": {
        "id": "3wg7I6c-ZO90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate the code to instantiate PathFinderDataModule\n",
        "\n",
        "choice = \"listops\"\n",
        "if choice == \"path_finder\":\n",
        "  dm = PathFinderDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default', num_workers=2, pin_memory=False, resolution=32,augment = False)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "\n",
        "\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # prompt: Generate the code to instantiate PathFinderDataModule\n",
        "  dm = PathfinderDataModule(data_dir='./', batch_size=32, test_batch_size=32)\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Fetch a sample from the training dataset\n",
        "  sample_idx = 0  # Index of the sample you want to print\n",
        "  sample = dm.train_dataset[sample_idx]\n",
        "\n",
        "  # If the sample is a tuple (image, label)\n",
        "  if isinstance(sample, tuple):\n",
        "      image, label = sample\n",
        "  else:\n",
        "      image = sample\n",
        "      label = None\n",
        "\n",
        "  # Print label\n",
        "  if label is not None:\n",
        "      print(f\"Label: {label}\")\n",
        "\n",
        "  # Print image\n",
        "  # Assuming image is a PIL image or a tensor that can be converted to PIL\n",
        "  if isinstance(image, torch.Tensor):\n",
        "      image = transforms.ToPILImage()(image)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"Sample {sample_idx}\")\n",
        "  plt.axis('off')  # Hide axis\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "elif choice == \"listops\":\n",
        "  dm = ListOpsDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default')\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Retrieve and print a sample\n",
        "  train_loader = dm.train_dataloader()\n",
        "\n",
        "  # Get a batch of data\n",
        "  for images, labels in train_loader:\n",
        "      print(f\"Batch of images shape: {images.shape}\")\n",
        "      print(f\"Batch of labels: {labels}\")\n",
        "\n",
        "      # Print the first image and label in the batch\n",
        "      print(f\"First image tensor: {images[0]}\")\n",
        "      print(f\"First label: {labels[0]}\")\n",
        "\n",
        "      # Break after first batch for demonstration\n",
        "      break\n",
        "\n",
        "elif choice == \"imdb\":\n",
        "  dm = IMDBDataModule(data_dir='./', batch_size=32, test_batch_size=32, data_type='default')\n",
        "  dm.prepare_data()\n",
        "  dm.setup()\n",
        "  # Retrieve and print a sample\n",
        "  train_loader = dm.train_dataloader()\n",
        "\n",
        "  # Get a batch of data\n",
        "  for images, labels in train_loader:\n",
        "      print(f\"Batch of images shape: {images.shape}\")\n",
        "      print(f\"Batch of labels: {labels}\")\n",
        "\n",
        "      # Print the first image and label in the batch\n",
        "      print(f\"First image tensor: {images[0]}\")\n",
        "      print(f\"First label: {labels[0]}\")\n",
        "\n",
        "      # Break after first batch for demonstration\n",
        "      break"
      ],
      "metadata": {
        "id": "Js7mYkgQ8pG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd8661b-aa2d-496b-e707-c826e3a87e66"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from datasets/tokenized_dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 7 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n",
            "<ipython-input-20-b7124aa67086>:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [torch.tensor(seq) for seq in input_ids], batch_first=True, padding_value=pad_value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of images shape: torch.Size([32, 512])\n",
            "Batch of labels: tensor([7, 2, 4, 6, 0, 0, 3, 2, 8, 8, 4, 9, 9, 1, 9, 9, 1, 8, 4, 6, 9, 0, 9, 8,\n",
            "        2, 2, 9, 7, 2, 1, 0, 7])\n",
            "First image tensor: tensor([ 1006.,  1006.,  1006.,  1031.,  4098.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031., 19960.,\n",
            "         1018.,  1007.,  1022.,  1007.,  1016.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1031.,  8117.,  1017.,  1007.,  1014.,  1007.,  1033.,  1007.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031.,  4098.,  1019.,\n",
            "         1007.,  1019.,  1007.,  1018.,  1007.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1031., 15488.,  1019.,  1007.,\n",
            "         1023.,  1007.,  1015.,  1007.,  1021.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1031.,  8117.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1031., 19960.,  1015.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031., 19960.,  1018.,\n",
            "         1007.,  1018.,  1007.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031.,\n",
            "        19960.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031.,  8117.,\n",
            "         1015.,  1007.,  1016.,  1007.,  1017.,  1007.,  1022.,  1007.,  1021.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1022.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031., 19960.,  1021.,\n",
            "         1007.,  1017.,  1007.,  1021.,  1007.,  1023.,  1007.,  1014.,  1007.,\n",
            "         1020.,  1007.,  1016.,  1007.,  1016.,  1007.,  1033.,  1007.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031.,  8117.,  1020.,\n",
            "         1007.,  1020.,  1007.,  1014.,  1007.,  1022.,  1007.,  1019.,  1007.,\n",
            "         1033.,  1007.,  1007.,  1033.,  1007.,  1007.,  1023.,  1007.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1031.,  8117.,  1018.,  1007.,  1020.,\n",
            "         1007.,  1014.,  1007.,  1019.,  1007.,  1033.,  1007.,  1007.,  1033.,\n",
            "         1007.,  1007.,  1023.,  1007.,  1023.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1031.,  4098.,  1023.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1031.,  4098.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1031., 15488.,  1016.,  1007.,  1014.,  1007.,  1016.,  1007.,  1019.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1018.,  1007.,  1022.,  1007.,  1015.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1018.,  1007.,  1021.,  1007.,  1033.,\n",
            "         1007.,  1007.,  1022.,  1007.,  1015.,  1007.,  1017.,  1007.,  1022.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1018.,  1007.,  1021.,  1007.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1031., 15488.,  1019.,  1007.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1031., 15488.,  1022.,  1007.,  1018.,  1007.,  1020.,  1007.,\n",
            "         1018.,  1007.,  1033.,  1007.,  1007.,  1017.,  1007.,  1020.,  1007.,\n",
            "         1023.,  1007.,  1015.,  1007.,  1020.,  1007.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1031.,  4098.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1031.,  8117.,  1019.,  1007.,  1017.,  1007.,  1022.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1031., 15488.,  1022.,  1007.,  1019.,  1007.,\n",
            "         1018.,  1007.,  1015.,  1007.,  1023.,  1007.,  1021.,  1007.,  1019.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1031., 15488.,  1020.,  1007.,  1021.,  1007.,  1017.,  1007.,  1014.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1021.,  1007.,  1016.,  1007.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031.,\n",
            "         8117.,  1006.,  1006.,  1006.,  1006.,  1006.,  1031., 19960.,  1023.,\n",
            "         1007.,  1018.,  1007.,  1019.,  1007.,  1022.,  1007.,  1033.,  1007.,\n",
            "         1007.,  1014.,  1007.,  1016.,  1007.,  1016.,  1007.,  1016.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,  1006.,\n",
            "         1006.,  1031.,  8117.,  1022.,  1007.,  1014.,  1007.,  1022.,  1007.,\n",
            "         1023.,  1007.,  1022.,  1007.,  1014.,  1007.,  1021.,  1007.,  1015.,\n",
            "         1007.,  1019.,  1007.,  1033.,  1007.,  1007.,  1023.,  1007.,  1020.,\n",
            "         1007.,  1033.,  1007.,  1007.,  1014.,  1007.,  1033.,  1007.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1006.,  1031.,  8117.,  1020.,  1007.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1031.,  4098.,  1021.,  1007.,  1006.,\n",
            "         1006.,  1006.,  1006.,  1006.,  1031., 19960.,  1021.,  1007.,  1020.,\n",
            "         1007.,  1022.,  1007.,  1023.,  1007.,  1033.,   102., 30522.])\n",
            "First label: 7\n"
          ]
        }
      ]
    }
  ]
}