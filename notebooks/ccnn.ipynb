{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "os2b6Zz0_t8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "DsSIWS3P_xQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v33j9vQ-uGS"
      },
      "source": [
        "# CCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8YvV-11-uGU"
      },
      "source": [
        "General imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFsSglnT-uGV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchmetrics\n",
        "import math\n",
        "import os\n",
        "import sklearn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.profilers import PyTorchProfiler\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, ModelSummary\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch import optim\n",
        "from typing import Optional\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import STL10\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWo2sUwE-uGW"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dRaDgpA-uGW"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjS-U7z-uGW"
      },
      "outputs": [],
      "source": [
        "def split_data(tensor, stratify):\n",
        "    # 0.7/0.15/0.15 train/val/test split\n",
        "    (\n",
        "        train_tensor,\n",
        "        testval_tensor,\n",
        "        train_stratify,\n",
        "        testval_stratify,\n",
        "    ) = sklearn.model_selection.train_test_split(\n",
        "        tensor,\n",
        "        stratify,\n",
        "        train_size=0.7,\n",
        "        random_state=0,\n",
        "        shuffle=True,\n",
        "        stratify=stratify,\n",
        "    )\n",
        "\n",
        "    val_tensor, test_tensor = sklearn.model_selection.train_test_split(\n",
        "        testval_tensor,\n",
        "        train_size=0.5,\n",
        "        random_state=1,\n",
        "        shuffle=True,\n",
        "        stratify=testval_stratify,\n",
        "    )\n",
        "    return train_tensor, val_tensor, test_tensor\n",
        "\n",
        "def save_data(dir, **tensors):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    for tensor_name, tensor_value in tensors.items():\n",
        "        torch.save(tensor_value, str(dir + \"/\" + tensor_name) + \".pt\")\n",
        "\n",
        "\n",
        "def load_data(dir):\n",
        "    tensors = {}\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith(\".pt\"):\n",
        "            tensor_name = filename.split(\".\")[0]\n",
        "            tensor_value = torch.load(str(dir + \"/\" + filename))\n",
        "            tensors[tensor_name] = tensor_value\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def load_data_from_partition(data_loc, partition):\n",
        "    assert partition in [\"train\", \"val\", \"test\"]\n",
        "    # load tensors\n",
        "    tensors = load_data(data_loc)\n",
        "    # select partition\n",
        "    name_x, name_y = f\"{partition}_x\", f\"{partition}_y\"\n",
        "    x, y = tensors[name_x], tensors[name_y]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def normalise_data(X, y):\n",
        "    train_X, _, _ = split_data(X, y)\n",
        "    out = []\n",
        "    for Xi, train_Xi in zip(X.unbind(dim=-1), train_X.unbind(dim=-1)):\n",
        "        train_Xi_nonan = train_Xi.masked_select(~torch.isnan(train_Xi))\n",
        "        mean = train_Xi_nonan.mean()  # compute statistics using only training data.\n",
        "        std = train_Xi_nonan.std()\n",
        "        out.append((Xi - mean) / (std + 1e-5))\n",
        "    out = torch.stack(out, dim=-1)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBPDXJI-uGW"
      },
      "source": [
        "MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu43a0xb-uGX"
      },
      "outputs": [],
      "source": [
        "class MnistDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, cfg, data_dir : str = \"../datasets\"):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.type = cfg.data.dataset\n",
        "        self.cfg = cfg\n",
        "        self.num_workers = 7\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "\n",
        "    def _set_transform(self):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.view(1, -1)) # flatten the image to 784 pixels\n",
        "        ])\n",
        "\n",
        "        if self.type == \"pmnist\":\n",
        "            self.transform.transforms.append(transforms.Lambda(lambda x: x[torch.randperm(self.size)]))  # permutation of the 784 pixels\n",
        "\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "        hidden_channels = self.cfg.net.hidden_channels\n",
        "\n",
        "        OmegaConf.update(self.cfg, \"train.batch_size\", 100)\n",
        "        OmegaConf.update(self.cfg, \"train.epochs\", 210)\n",
        "        OmegaConf.update(self.cfg, \"net.in_channels\", 1)\n",
        "        OmegaConf.update(self.cfg, \"net.out_channels\", 10)\n",
        "        OmegaConf.update(self.cfg, \"net.data_dim\", 1)\n",
        "\n",
        "        if hidden_channels == 140:\n",
        "            if self.type == \"smnist\":\n",
        "                OmegaConf.update(self.cfg, \"train.learning_rate\", 0.01)\n",
        "                OmegaConf.update(self.cfg, \"train.dropout_rate\", 0.1)\n",
        "                OmegaConf.update(self.cfg, \"train.weight_decay\", 1e-6)\n",
        "                OmegaConf.update(self.cfg, \"kernel.omega_0\", 2976.49)\n",
        "            elif self.type == \"pmnist\":\n",
        "                OmegaConf.update(self.cfg, \"train.learning_rate\", 0.02)\n",
        "                OmegaConf.update(self.cfg, \"train.dropout_rate\", 0.2)\n",
        "                OmegaConf.update(self.cfg, \"train.weight_decay\", 0)\n",
        "                OmegaConf.update(self.cfg, \"kernel.omega_0\", 2985.63)\n",
        "        elif hidden_channels == 380:\n",
        "            OmegaConf.update(self.cfg, \"train.weight_decay\", 0)\n",
        "\n",
        "            if self.type == \"smnist\":\n",
        "                OmegaConf.update(self.cfg, \"train.learning_rate\", 0.01)\n",
        "                OmegaConf.update(self.cfg, \"train.dropout_rate\", 0.1)\n",
        "                OmegaConf.update(self.cfg, \"kernel.omega_0\", 2976.49)\n",
        "            elif self.type == \"pmnist\":\n",
        "                OmegaConf.update(self.cfg, \"train.learning_rate\", 0.02)\n",
        "                OmegaConf.update(self.cfg, \"train.dropout_rate\", 0.2)\n",
        "                OmegaConf.update(self.cfg, \"kernel.omega_0\", 2985.63)\n",
        "\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        self._set_transform()\n",
        "        self._yaml_parameters()\n",
        "\n",
        "        self.batch_size = self.cfg.train.batch_size\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\":\n",
        "            self.mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(\n",
        "                self.mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
        "            )\n",
        "            print(f'Training set size: {len(self.mnist_train)}')\n",
        "            print(f'Validation set size: {len(self.mnist_val)}')\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\":\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "            print(f'Test set size: {len(self.mnist_test)}')\n",
        "\n",
        "        if stage == \"predict\":\n",
        "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "            print(f'Prediction set size: {len(self.mnist_predict)}')\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.mnist_predict,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6l1Y2ag-uGX"
      },
      "source": [
        "SpeechCommands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TchPqKD2-uGX"
      },
      "outputs": [],
      "source": [
        "class SpeechCommandsModule(pl.LightningDataModule):\n",
        "    def __init__(self, cfg, data_dir : str = \"../datasets\"):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.data_processed_location = self.data_dir + \"/SpeechCommands/processed_data\"\n",
        "        self.download_location = self.data_dir + \"/SpeechCommands/speech_commands_v0.02\"\n",
        "        self.type = cfg.data.dataset\n",
        "        self.cfg = cfg\n",
        "        self.num_workers = 7\n",
        "\n",
        "\n",
        "    def process_data(self):\n",
        "        x = torch.empty(34975, 16000, 1)\n",
        "        y = torch.empty(34975, dtype=torch.long)\n",
        "\n",
        "        batch_index = 0\n",
        "        y_index = 0\n",
        "        for foldername in (\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ):\n",
        "            loc = self.download_location + \"/\" + foldername\n",
        "            for filename in tqdm(os.listdir(loc)):\n",
        "                audio, _ = torchaudio.load(\n",
        "                    loc + \"/\" + filename,\n",
        "                    channels_first=False,\n",
        "                )\n",
        "\n",
        "                # A few samples are shorter than the full length; for simplicity we discard them.\n",
        "                if len(audio) != 16000:\n",
        "                    continue\n",
        "\n",
        "                x[batch_index] = audio\n",
        "                y[batch_index] = y_index\n",
        "                batch_index += 1\n",
        "            y_index += 1\n",
        "\n",
        "\n",
        "        # If MFCC, then we compute these coefficients.\n",
        "        if self.type == \"sc_mfcc\":\n",
        "            x = torchaudio.transforms.MFCC(\n",
        "                log_mels=True, n_mfcc=20, melkwargs=dict(n_fft=200, n_mels=64)\n",
        "            )(x.squeeze(-1)).detach()\n",
        "            # X is of shape (batch=34975, channels=20, length=161)\n",
        "        else:\n",
        "            x = x.unsqueeze(1).squeeze(-1)\n",
        "            # X is of shape (batch=34975, channels=1, length=16000)\n",
        "\n",
        "        # Normalize data\n",
        "        if self.type == \"sc_mfcc\":\n",
        "            x = normalise_data(x.transpose(1, 2), y).transpose(1, 2)\n",
        "        else:\n",
        "            x = normalise_data(x, y)\n",
        "\n",
        "        train_x, val_x, test_x = split_data(x, y)\n",
        "        train_y, val_y, test_y = split_data(y, y)\n",
        "\n",
        "\n",
        "        return (\n",
        "            train_x,\n",
        "            val_x,\n",
        "            test_x,\n",
        "            train_y,\n",
        "            val_y,\n",
        "            test_y,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        SPEECHCOMMANDS(self.data_dir, download=True)\n",
        "        if not os.path.exists(self.data_processed_location + \"/train_x.pt\"):\n",
        "            train_x, val_x, test_x, train_y, val_y, test_y = self.process_data()\n",
        "\n",
        "            save_data(\n",
        "                self.data_processed_location,\n",
        "                train_x=train_x,\n",
        "                val_x=val_x,\n",
        "                test_x=test_x,\n",
        "                train_y=train_y,\n",
        "                val_y=val_y,\n",
        "                test_y=test_y,\n",
        "            )\n",
        "\n",
        "\n",
        "    def _yaml_parameters(self):\n",
        "        OmegaConf.update(self.cfg, \"net.out_channels\", 10)\n",
        "        OmegaConf.update(self.cfg, \"net.data_dim\", 1)\n",
        "        OmegaConf.update(self.cfg, \"train.dropout_rate\", 0.2)\n",
        "        OmegaConf.update(self.cfg, \"train.learning_rate\", 0.02)\n",
        "        OmegaConf.update(self.cfg, \"train.weight_decay\", 1e-6)\n",
        "\n",
        "        # 140 and 380 hidden_channels have same parameters\n",
        "        if self.type == \"sc_raw\":\n",
        "            OmegaConf.update(self.cfg, \"net.in_channels\", 1)\n",
        "            OmegaConf.update(self.cfg, \"train.batch_size\", 20)\n",
        "            OmegaConf.update(self.cfg, \"train.epochs\", 160)\n",
        "            OmegaConf.update(self.cfg, \"kernel.omega_0\", 1295.61)\n",
        "        elif self.type == \"sc_mfcc\":\n",
        "            OmegaConf.update(self.cfg, \"net.in_channels\", )\n",
        "            OmegaConf.update(self.cfg, \"train.batch_size\", 100)\n",
        "            OmegaConf.update(self.cfg, \"train.epochs\", 110)\n",
        "            OmegaConf.update(self.cfg, \"kernel.omega_0\", 750.18)\n",
        "\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        self._yaml_parameters()\n",
        "\n",
        "        self.batch_size = self.cfg.train.batch_size\n",
        "\n",
        "        if stage == \"fit\":\n",
        "            # train\n",
        "            x_train, y_train = load_data_from_partition(\n",
        "                self.data_processed_location, partition=\"train\"\n",
        "            )\n",
        "            self.train_dataset = TensorDataset(x_train, y_train)\n",
        "            # validation\n",
        "            x_val, y_val = load_data_from_partition(\n",
        "                self.data_processed_location, partition=\"val\"\n",
        "            )\n",
        "            self.val_dataset = TensorDataset(x_val, y_val)\n",
        "        if stage == \"test\":\n",
        "            # test\n",
        "            x_test, y_test = load_data_from_partition(\n",
        "                self.data_processed_location, partition=\"test\"\n",
        "            )\n",
        "            self.test_dataset = TensorDataset(x_test, y_test)\n",
        "        if stage == \"predict\":\n",
        "            # predict\n",
        "            x_test, y_test = load_data_from_partition(\n",
        "                self.data_processed_location, partition=\"test\"\n",
        "            )\n",
        "            self.test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go-mZdJj-uGY"
      },
      "source": [
        "get_data_module function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WlaAWOw-uGY"
      },
      "outputs": [],
      "source": [
        "def get_data_module(cfg : OmegaConf):\n",
        "\n",
        "    assert cfg.data.dataset in [\"smnist\",\"pmnist\",\"cifar10\",\"scifar10\",\"cifar100\",\"stl10\",\"sc_mfcc\",\"sc_raw\",\"pathfinder\",\"path_x\",\"image\"], \"Dataset not supported\"\n",
        "\n",
        "    # can be either sequential or permuted mnist\n",
        "    if \"mnist\" in cfg.data.dataset:\n",
        "        return MnistDataModule(cfg)\n",
        "    # if cfg.data.dataset == \"cifar10\" or cfg.data.dataset == \"scifar10\":\n",
        "    #     return Cifar10DataModule(cfg)\n",
        "    # if cfg.data.dataset == \"cifar100\":\n",
        "    #     return Cifar100DataModule(cfg)\n",
        "    # if cfg.data.dataset == \"stl10\":\n",
        "    #     return STL10DataModule(cfg)\n",
        "    if cfg.data.dataset == \"sc_mfcc\" or cfg.data.dataset == \"sc_raw\":\n",
        "        return SpeechCommandsModule(cfg)\n",
        "\n",
        "    # TODO other dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qd9DKqf-uGY"
      },
      "source": [
        "# CCNN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMLHi_ZP-uGY"
      },
      "source": [
        "GetBatchNormalization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSvbuoud-uGY"
      },
      "outputs": [],
      "source": [
        "def GetBatchNormalization(data_dim, num_features):\n",
        "    if data_dim == 1:\n",
        "        return nn.BatchNorm1d(num_features)\n",
        "    elif data_dim == 2:\n",
        "        return nn.BatchNorm2d(num_features)\n",
        "    elif data_dim == 3:\n",
        "        return nn.BatchNorm3d(num_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faPmd3TK-uGY"
      },
      "source": [
        "GetDropout function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh_2TFuw-uGY"
      },
      "outputs": [],
      "source": [
        "def GetDropout(data_dim):\n",
        "    if data_dim == 1:\n",
        "        return nn.Dropout1d()\n",
        "    elif data_dim == 2:\n",
        "        return nn.Dropout2d()\n",
        "    elif data_dim == 3:\n",
        "        return nn.Dropout3d()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwRLl6HW-uGZ"
      },
      "source": [
        "GetAdaptiveAvgPool function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqMXRK1N-uGZ"
      },
      "outputs": [],
      "source": [
        "def GetAdaptiveAvgPool(data_dim, output_size):\n",
        "    if data_dim == 1:\n",
        "        return nn.AdaptiveAvgPool1d(output_size)\n",
        "    elif data_dim == 2:\n",
        "        return nn.AdaptiveAvgPool2d(output_size)\n",
        "    elif data_dim == 3:\n",
        "        return nn.AdaptiveAvgPool3d(output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWq3ix8m-uGZ"
      },
      "source": [
        "create_coordinates function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osJySnF9-uGZ"
      },
      "outputs": [],
      "source": [
        "def create_coordinates(kernel_size, data_dim):\n",
        "\n",
        "    values = torch.linspace(-1, 1, steps=kernel_size)   # i.e tensor([-1, 1])\n",
        "    positions = [values for _ in range(data_dim)]      # i.e [tensor([-1, 1]), tensor([-1, 1])]\n",
        "\n",
        "    grids = []\n",
        "    for i, t in enumerate(positions):\n",
        "        shape = [1] * data_dim      # i.e [1, 1] for data_dim = 2\n",
        "        shape[i] = -1               # shape = [-1, 1] i = 0\n",
        "                                    # shape = [1, -1] i = 1\n",
        "\n",
        "        t_reshaped = t.view(*shape) # t_reshaped_0 [3, 1], t_reshaped_1 [1, 3]\n",
        "\n",
        "        t_broadcasted = t_reshaped.expand(* [kernel_size] * data_dim) # expand dimension to match [3,3]\n",
        "        grids.append(t_broadcasted)\n",
        "\n",
        "    grids = torch.stack(grids, dim=0).unsqueeze(0)  # stack along a new dimension [2,3,3] and another dimension [1,2,3,3]\n",
        "\n",
        "    return grids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCZ2qLfX-uGZ"
      },
      "source": [
        "Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQfjn2KA-uGZ"
      },
      "outputs": [],
      "source": [
        "class LinearLayer(nn.Module):\n",
        "    def __init__(self, dim: int, in_channels: int, out_channels: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        if dim == 1:\n",
        "            self.layer = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=bias)\n",
        "        elif dim == 2:\n",
        "            self.layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=bias)\n",
        "        elif dim == 3:\n",
        "            self.layer = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, bias=bias)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid dimension {dim}. Supported dimensions are 1, 2, and 3.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsnnN8-b-uGZ"
      },
      "source": [
        "Anisotropic Gabor Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv250SYv-uGZ"
      },
      "outputs": [],
      "source": [
        "class AnisotropicGaborLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dim: int,\n",
        "        hidden_channels: int,\n",
        "        current_layer: int,\n",
        "        omega_0: float = 2976.49,\n",
        "        alpha: float = 6.0,\n",
        "        beta: float = 1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        # linear layer\n",
        "        self.linear = LinearLayer(\n",
        "            dim=data_dim,\n",
        "            in_channels=data_dim,\n",
        "            out_channels=hidden_channels,\n",
        "            bias=True,\n",
        "        )\n",
        "\n",
        "        gamma_dist = torch.distributions.gamma.Gamma(alpha / (current_layer + 1), beta)\n",
        "\n",
        "        # generate as gamma_dist as data_dim (gamma_x, gamma_y, ...)\n",
        "        self.gamma = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(gamma_dist.sample((hidden_channels, 1)))\n",
        "                for _ in range(data_dim)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        normal_dist = torch.distributions.normal.Normal(0, 1)\n",
        "\n",
        "        # generate as many mi as data_dim (mi_x, mi_y, ...)\n",
        "        self.mi = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(normal_dist.sample((hidden_channels, 1)))\n",
        "                for _ in range(data_dim)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        scaling_factor = 25.6\n",
        "\n",
        "        self.linear.weight = nn.Parameter(torch.randn(hidden_channels,data_dim,*((1,) * data_dim)))\n",
        "        self.linear.weight.data *= omega_0 * scaling_factor * self.gamma[0].view(\n",
        "            *self.gamma[0].shape, *((1,) * data_dim)\n",
        "        )\n",
        "\n",
        "        self.linear.bias = nn.Parameter(torch.randn(hidden_channels))\n",
        "        self.linear.bias.data.uniform_(-np.pi, np.pi)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # coordinates (x,y,...)\n",
        "        coord = [x[0][i] for i in range(self.data_dim)]\n",
        "\n",
        "        # reshaping the parameters to [1, 1, 1, W, H] if data_dim = 2\n",
        "        reshaped_coord = [c.view(1, 1, 1, *c.shape) for c in coord]\n",
        "\n",
        "        reshaped_gamma = [\n",
        "            g.view(1, *g.shape, *((1,) * (self.data_dim))) for g in self.gamma\n",
        "        ]\n",
        "\n",
        "        reshaped_mi = [m.view(1, *m.shape, *((1,) * (self.data_dim))) for m in self.mi]\n",
        "        # -> [1, hidden_channels, 1, 1, 1] if data_dim = 2\n",
        "\n",
        "        g_envelopes = []\n",
        "        for i in range(self.data_dim):\n",
        "            g_envelope = torch.exp(\n",
        "                -0.5 * (reshaped_gamma[i] * (reshaped_coord[i] - reshaped_mi[i])) ** 2\n",
        "            )  # Shape: [1, hidden_channels, 20, 20]\n",
        "            g_envelopes.append(g_envelope)\n",
        "\n",
        "        # Multiply all the envelopes together\n",
        "        g_envelope = g_envelopes[0]\n",
        "        for i in range(1, self.data_dim):\n",
        "            g_envelope *= g_envelopes[i]\n",
        "\n",
        "        # Squeeze the third dimension\n",
        "        g_envelope = g_envelope.squeeze(2)\n",
        "\n",
        "        # computing the sinusoidal\n",
        "        sinusoidal = torch.sin(self.linear(x))\n",
        "\n",
        "        return g_envelope * sinusoidal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhJf5W7j-uGa"
      },
      "source": [
        "MFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_-iLPfL-uGa"
      },
      "outputs": [],
      "source": [
        "class MFN(nn.Module):\n",
        "    def __init__(\n",
        "        self, data_dim: int, hidden_channels: int, out_channels: int, no_layers: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the MFN class.\n",
        "        Args:\n",
        "            data_dim (int): The dimension of the input data.\n",
        "            hidden_channels (int): The number of hidden channels in the linear layers.\n",
        "            out_channels (int): The number of output channels in the final linear layer.\n",
        "            no_layers (int): The number of hidden layers in the network.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(MFN, self).__init__()\n",
        "\n",
        "        # hidden layers\n",
        "        self.linearLayer = nn.ModuleList(\n",
        "            [\n",
        "                LinearLayer(\n",
        "                    dim=data_dim,\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    bias=True,\n",
        "                )\n",
        "                for _ in range(no_layers - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # output layer\n",
        "        self.linearLayer.append(\n",
        "            LinearLayer(\n",
        "                dim=data_dim,\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "                bias=True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.reweighted_output_layer = False\n",
        "\n",
        "    def re_weight_output_layer(self, kernel_positions: torch.Tensor, in_channels: int, data_dim: int):\n",
        "        \"\"\"\n",
        "        Re-weights the last layer of the kernel net by factor = gain / sqrt(in_channels * kernel_size).\n",
        "        Args:\n",
        "            gain (float): The gain to re-weight the last layer by.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.reweighted_output_layer:\n",
        "\n",
        "            # Re weight the last layer of the kernel net\n",
        "\n",
        "            kernel_size = torch.Tensor([*kernel_positions.shape[data_dim:]]).prod().item() # just a way to get the kernel size\n",
        "            # [1, 2, 33, 33] -> [33,33] for data_dim=2\n",
        "            # prod multiplies all elements in the tensor i.e. 33*33 = 1089\n",
        "            # item converts the tensor to a python number\n",
        "\n",
        "            # define gain / sqrt(in_channels * kernel_size) by Chang et al. (2020)\n",
        "            factor = 1.0 / math.sqrt(in_channels * kernel_size)\n",
        "\n",
        "            # get the last layer and re-weight it\n",
        "            self.linearLayer[-1].layer.weight.data *= factor\n",
        "\n",
        "            # set the flag to True so that the output layer is only re-weighted the first time\n",
        "            self.reweighted_output_layer = True\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = self.gabor_filters[0](x)\n",
        "        for l in range(1, len(self.gabor_filters)):\n",
        "            h = self.gabor_filters[l](x) * self.linearLayer[l - 1](h)\n",
        "\n",
        "        last = self.linearLayer[-1](h)\n",
        "\n",
        "        return last"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_0E8Vv6-uGa"
      },
      "source": [
        "MAGNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSEBkI_3-uGa"
      },
      "outputs": [],
      "source": [
        "class MAGNet(MFN):\n",
        "    def __init__(\n",
        "        self, data_dim: int, hidden_channels: int, out_channels: int, no_layers: int, omega_0: float\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TODO\n",
        "        \"\"\"\n",
        "        super().__init__(data_dim, hidden_channels, out_channels, no_layers)\n",
        "        self.gabor_filters = nn.ModuleList(\n",
        "            [\n",
        "                AnisotropicGaborLayer(\n",
        "                    data_dim=data_dim,\n",
        "                    hidden_channels=hidden_channels,\n",
        "                    current_layer=l,\n",
        "                    omega_0=omega_0,\n",
        "                )\n",
        "                for l in range(no_layers)\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJJH5LP-uGa"
      },
      "source": [
        "Conv functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA5HScnY-uGa"
      },
      "outputs": [],
      "source": [
        "def conv1d(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor],\n",
        "    padding: int,\n",
        "    groups: int,\n",
        "):\n",
        "    return torch.nn.functional.conv1d(x, kernel, bias=bias, padding=padding, stride=1, groups=groups)\n",
        "\n",
        "def conv2d(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor],\n",
        "    padding: int,\n",
        "    groups: int,\n",
        "):\n",
        "    return torch.nn.functional.conv2d(x, kernel, bias=bias, padding=padding, stride=1, groups=groups)\n",
        "\n",
        "def conv3d(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor],\n",
        "    padding: int,\n",
        "    groups: int,\n",
        "):\n",
        "    return torch.nn.functional.conv3d(x, kernel, bias=bias, padding=padding, stride=1, groups=groups)\n",
        "\n",
        "\n",
        "def get_conv_function(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor],\n",
        "    padding: int,\n",
        "    groups: int,\n",
        "    dim: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns the Convolutional Layer.\n",
        "    \"\"\"\n",
        "    if dim == 1:\n",
        "        # x = F.pad(x, [padding[0], padding[0]], value=0.0)\n",
        "        return conv1d(x, kernel, bias, padding, groups)\n",
        "    elif dim == 2:\n",
        "        return conv2d(x, kernel, bias, padding, groups)\n",
        "    elif dim == 3:\n",
        "        return conv3d(x, kernel, bias, padding, groups)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid dimension {dim}\")\n",
        "\n",
        "\n",
        "def conv(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor] = None,\n",
        "):\n",
        "\n",
        "    data_dim = len(x.shape) - 2\n",
        "    # -> [batch_size, channels, x_dimension, y_dimension, ...] -> len[x.shape] = 2 + data_dim\n",
        "\n",
        "    kernel_size = torch.tensor(kernel.shape[-data_dim:])\n",
        "    assert torch.all(\n",
        "        kernel_size % 2 != 0\n",
        "    ), f\"Convolutional kernels must have odd dimensionality. Received {kernel.shape}\"\n",
        "    # pad by kernel_size // 2 so that the output has the same size as the input\n",
        "    padding = (kernel_size // 2).tolist()\n",
        "\n",
        "    groups = kernel.shape[1]\n",
        "    # invert first two dimensions of kernel because there should be one kernel per input channel\n",
        "    kernel = kernel.view(kernel.shape[1], 1, *kernel.shape[2:])\n",
        "\n",
        "    return get_conv_function(x, kernel, bias, padding=padding, groups=groups, dim=data_dim)\n",
        "\n",
        "\n",
        "def fftconv(\n",
        "    x: torch.Tensor,\n",
        "    kernel: torch.Tensor,\n",
        "    bias: Optional[torch.Tensor] = None,\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    data_dim = len(x.shape) - 2\n",
        "    # -> [batch_size, channels, x_dimension, y_dimension, ...] -> len[x.shape] = 2 + data_dim\n",
        "\n",
        "    assert data_dim == 1\n",
        "\n",
        "    kernel_size = torch.tensor(kernel.shape[-data_dim:])\n",
        "    assert torch.all(\n",
        "        kernel_size % 2 != 0\n",
        "    ), f\"Convolutional kernels must have odd dimensionality. Received {kernel.shape}\"\n",
        "\n",
        "    # padding input\n",
        "    padding_x = kernel.shape[-1] // 2\n",
        "    padding_x = (2 * data_dim) * [padding_x]\n",
        "\n",
        "    x_padded = F.pad(x, padding_x)\n",
        "\n",
        "    if x_padded.shape[-1] % 2 != 0:\n",
        "        x_padded = F.pad(x_padded, [0, 1])\n",
        "\n",
        "    # padding kernel\n",
        "    padding_kernel = [\n",
        "        pad\n",
        "        for i in reversed(range(2, x_padded.ndim))\n",
        "        for pad in [0, x_padded.shape[i] - kernel.shape[i]]\n",
        "    ]\n",
        "\n",
        "    kernel_padded = F.pad(kernel, padding_kernel, mode=\"constant\", value=0)\n",
        "\n",
        "    # Fourier Transform\n",
        "    x_fr = torch.fft.rfftn(x_padded, dim=tuple(range(2, x_padded.ndim)))\n",
        "    kernel_fr = torch.fft.rfftn(kernel_padded, dim=tuple(range(2, kernel.ndim)))\n",
        "\n",
        "    # (Input * Conj(Kernel)) = Correlation(Input, Kernel)\n",
        "    # b->batch i->input channel o->output channel , ...> any additional dimensions\n",
        "    # The einsum notation specifies that for each position in the output tensor,\n",
        "    # you perform element-wise multiplication of x_fr and kernel_fr and sum over the i dimension (input channels)\n",
        "    # and any remaining spatial dimensions\n",
        "    # Essentially, it calculates a form of convolution (correlation) in the Fourier domain,\n",
        "    # where the i dimension of x_fr is multiplied with the i dimension of kernel_fr\n",
        "    # and summed to produce the output tensor's o dimension\n",
        "    kernel_fr = torch.conj(kernel_fr)\n",
        "    # Assuming x_fr has shape [batch_size, num_channels, x_dim1, x_dim2, ...]\n",
        "    # and kernel_fr has shape [num_channels, k_dim1, k_dim2, ...]\n",
        "    print(\n",
        "        f\"expected_shape : [batch_size, num_channels, x_dim1, x_dim2, ...] , x_fr shape: {x_fr.shape}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"expected_shape : [num_channels, k_dim1, k_dim2, ...] ,kernel_fr shape: {kernel_fr.shape}\"\n",
        "    )\n",
        "    # Element-wise Multiplication in Fourier domain\n",
        "    output_fr = x_fr * kernel_fr\n",
        "\n",
        "    # Inverse FFT to transform the result back to the spatial domain\n",
        "    out = torch.fft.irfftn(output_fr, dim=tuple(range(2, x_padded.ndim))).float()\n",
        "\n",
        "    # This part of the code ensures that the output tensor out has the same spatial dimensions as the original input tensor x (before padding)\n",
        "\n",
        "    # Select all elements in the batch_size and channels dimensions (first two dimensions of out)\n",
        "    slices = [slice(None), slice(None)]\n",
        "    # Extension of the slices list to include slices for each spatial dimension (for all dimensions [data_dim])\n",
        "    # Let's assume x_padded has a shape of [batch_size, channels, height, width]. After this step, slices might look like:\n",
        "    # - slices = [slice(None), slice(None), slice(None, height), slice(None, width)]\n",
        "    slices.extend(slice(None, x.shape[-i]) for i in range(1, data_dim + 1))\n",
        "    # This operation effectively crops the out tensor to remove any padding that was added during the earlier steps\n",
        "    out = out[tuple(slices)]\n",
        "\n",
        "    # Add bias if provided\n",
        "    if bias is not None:\n",
        "        out = out + bias.view(1, -1, *([1] * data_dim))\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVjZ1yuv-uGb"
      },
      "source": [
        "SeparableFlexConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24t8638B-uGb"
      },
      "outputs": [],
      "source": [
        "class SepFlexConv(nn.Module):\n",
        "    \"\"\"\n",
        "    SeparableFlexConv (SepFlexConv) is a depthwise separable version of FlexConv (Romero et al., 2022a)\n",
        "\n",
        "    ConstructMaskedKernel is a continuous version of the kernel whichs multiplied by a Gaussian mask.\n",
        "\n",
        "    The gaussian mask has learnable parameters and by learning it the model can learn the size of the convolutional kernel.\n",
        "\n",
        "    The flow is the following:\n",
        "\n",
        "        input\n",
        "            |\n",
        "            |\n",
        "            |\n",
        "            -------------- |\n",
        "            |              |input.length\n",
        "            |              |\n",
        "            |    ConstructMaskedKernel\n",
        "            |              |\n",
        "            |              |\n",
        "            SpatialConvolution\n",
        "                    |\n",
        "                    |\n",
        "            DepthwiseConvolution\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dim: int,\n",
        "        in_channels: int,\n",
        "        net_cfg: OmegaConf,\n",
        "        kernel_cfg: OmegaConf,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the CKConv module.\n",
        "        Args:\n",
        "            data_dim (int): The dimensionality of the input data.\n",
        "            in_channels (int): The number of input channels.\n",
        "            out_channels (int): The number of output channels.\n",
        "            hidden_channels (int): The number of hidden channels.\n",
        "            kernel_no_layers (int): The number of layers in the kernel network.\n",
        "            kernel_hidden_channels (int): The number of hidden channels in the kernel network.\n",
        "            kernel_size (int, optional): The size of the kernel. Defaults to 33.\n",
        "            conv_type (str, optional): The type of convolution. Defaults to \"conv\".\n",
        "            fft_thresold (int, optional): The threshold for using FFT. Defaults to 50.\n",
        "            bias (bool, optional): Whether to include bias in the pointwise convolution layer. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # sep flex conv parameters\n",
        "        self.data_dim = data_dim\n",
        "        self.in_channels = in_channels\n",
        "        hidden_channels = net_cfg.hidden_channels\n",
        "\n",
        "\n",
        "        # kernel parameters\n",
        "        kernel_no_layers = kernel_cfg.kernel_no_layers\n",
        "        kernel_hidden_channels = kernel_cfg.kernel_hidden_channels\n",
        "        self.kernel_size = kernel_cfg.kernel_size\n",
        "        self.conv_type = kernel_cfg.conv_type\n",
        "        self.fft_threshold = kernel_cfg.fft_threshold\n",
        "\n",
        "\n",
        "        # init relative positions of the kernel\n",
        "        self.kernel_positions = torch.zeros(1)\n",
        "\n",
        "        if net_cfg.bias:\n",
        "            # init random bias with in_channels dimensions\n",
        "            self.bias = torch.randn(in_channels)\n",
        "            self.bias.data.fill_(0.0)\n",
        "        else :\n",
        "            self.bias = None\n",
        "\n",
        "\n",
        "        # init gaussian mask parameter\n",
        "        self.mask_mean = torch.nn.Parameter(torch.zeros(data_dim)) # mi = 0\n",
        "        self.mask_sigma = torch.nn.Parameter(torch.ones(data_dim)) # sigma = 1\n",
        "\n",
        "\n",
        "        # Define the kernel net, in our case always a MAGNet\n",
        "        self.KernelNet = MAGNet(\n",
        "            data_dim=data_dim,\n",
        "            hidden_channels=kernel_hidden_channels,\n",
        "            out_channels=in_channels, # always in channel because separable\n",
        "            no_layers=kernel_no_layers,\n",
        "            omega_0=kernel_cfg.omega_0,\n",
        "        )\n",
        "\n",
        "        # Define the pointwise convolution layer (page 4 original paper)\n",
        "        self.pointwise_conv = LinearLayer(\n",
        "            dim=data_dim,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=hidden_channels,\n",
        "            bias=net_cfg.bias,\n",
        "        )\n",
        "\n",
        "\n",
        "    def construct_masked_kernel(self, x):\n",
        "        \"\"\"\n",
        "        Construct the masked kernel by multiplying the result of the kernel net with a\n",
        "        gaussian mask.\n",
        "\n",
        "        input.length\n",
        "        |\n",
        "        GetRelPositions\n",
        "        RelPositions\n",
        "        |\n",
        "        KernelNet\n",
        "        ConvKernel\n",
        "        |\n",
        "        GaussMask\n",
        "        |\n",
        "        MaskedKernel\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Get the relative positions\n",
        "        kernel_positions = self.get_rel_positions(x)\n",
        "\n",
        "        # 2 Re-weight the output layer of the kernel net\n",
        "        self.KernelNet.re_weight_output_layer(kernel_positions, self.in_channels, self.data_dim)\n",
        "\n",
        "        # 3. Get the kernel\n",
        "        conv_kernel = self.KernelNet(kernel_positions)\n",
        "\n",
        "        # 4. Get the mask gaussian mask\n",
        "        mask = self.gaussian_mask(\n",
        "            kernel_pos=kernel_positions,\n",
        "            mask_mean=self.mask_mean,\n",
        "            mask_sigma=self.mask_sigma,\n",
        "        )\n",
        "\n",
        "        return conv_kernel * mask\n",
        "\n",
        "    def get_rel_positions(self, x):\n",
        "        \"\"\"\n",
        "        Handles the vector or relative positions which is given to KernelNet.\n",
        "        \"\"\"\n",
        "        if (\n",
        "            self.kernel_positions.shape[-1] == 1  # Only for the first time\n",
        "        ):  # The conv. receives input signals of length > 1\n",
        "\n",
        "            # Creates the vector of relative positions\n",
        "\n",
        "            kernel_positions = create_coordinates(\n",
        "                kernel_size=self.kernel_size,\n",
        "                data_dim=self.data_dim,\n",
        "            )\n",
        "            # -> Grid sized: [kernel_size] * data_dim\n",
        "            # -> kernel_positions : [1, dim, kernel_size, kernel_size]\n",
        "\n",
        "            self.kernel_positions = kernel_positions.type_as(self.kernel_positions)\n",
        "            # -> With form: [batch_size=1, dim, x_dimension, y_dimension, ...]\n",
        "\n",
        "            # Save the step size for the calculation of dynamic cropping\n",
        "            # The step is max - min / (no_steps - 1)\n",
        "            # TODO : Check cropping\n",
        "            # self.linspace_stepsize = (\n",
        "            #     (1.0 - (-1.0)) / (self.train_length[0] - 1)\n",
        "            # ).type_as(self.linspace_stepsize)\n",
        "        return self.kernel_positions\n",
        "\n",
        "    def gaussian_mask(\n",
        "            self,\n",
        "            kernel_pos: torch.Tensor,\n",
        "            mask_mean: torch.Tensor,\n",
        "            mask_sigma: torch.Tensor\n",
        "        ) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Generates a Gaussian mask based on the given parameters.\n",
        "        Args:\n",
        "            kernel_pos (torch.Tensor): The position of the kernel.\n",
        "            mask_mean (torch.Tensor): The mean value of the mask.\n",
        "            mask_sigma (torch.Tensor): The standard deviation of the mask.\n",
        "            Returns:\n",
        "                torch.Tensor: The generated Gaussian mask of [1, 1, Y, X] in 2D or [1, 1, X] in 1D\n",
        "\n",
        "        Example 2D:\n",
        "            if kernel_pos.shape = [1, 2, 33, 33] and mask_mean.shape = [1, 2]\n",
        "            in order to sum them you need to reshape mask_mean to [1, 2, 1, 1]\n",
        "            Then you sum over the first dimension and the output will be [1, 1, 33, 33]\n",
        "        \"\"\"\n",
        "\n",
        "        # reshape the mask_mean and mask_sigma so that they can be broadcasted\n",
        "        mask_mean = mask_mean.view(1, self.data_dim, *(1,) * self.data_dim)\n",
        "        mask_sigma = mask_sigma.view(1, self.data_dim, *(1,) * self.data_dim)\n",
        "\n",
        "        return torch.exp(\n",
        "            -0.5\n",
        "            * (\n",
        "                1.0 / (mask_sigma**2) * (kernel_pos - mask_mean) ** 2\n",
        "            ).sum(1, keepdim=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the SepFlexConv model.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "        Example 2D:\n",
        "            1. x.shape = [64, 140, 32, 32]\n",
        "            2. masked_kernel.shape = [1, 140, 33, 33]\n",
        "            3. spatial convolution between x and masked kernel -> [64, 140, 32, 32]\n",
        "            4. Pointwise convolution -> [64, 140, 32, 32]\n",
        "        \"\"\"\n",
        "\n",
        "        masked_kernel = self.construct_masked_kernel(x)\n",
        "\n",
        "        size = torch.tensor(masked_kernel.shape[2:]) # -> [33,33] for data_dim=2\n",
        "        # fftconv is used when the size of the kernel is large enough\n",
        "        if self.conv_type == \"fftconv\" and torch.all(size > self.fft_thresold):\n",
        "            out = fftconv(x=x, kernel=masked_kernel, bias=self.bias)\n",
        "        else:\n",
        "            out = conv(x=x, kernel=masked_kernel, bias=self.bias)\n",
        "\n",
        "        # pointwise convolution where out is the spatial convolution\n",
        "        out = self.pointwise_conv(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_C4ucp2-uGb"
      },
      "source": [
        "S4Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJZDIPcr-uGb"
      },
      "outputs": [],
      "source": [
        "class S4Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Create a S4 block (Gu et al., 2022) as defined in the Continuous CNN architecture.\n",
        "\n",
        "          input\n",
        "            |\n",
        "    | -------------|\n",
        "    |            BarchNorm\n",
        "    |            SepFlecConv\n",
        "    |            GELU\n",
        "    |            DropOut\n",
        "    |            PointwiseLinear\n",
        "    |            GELU\n",
        "    |              |\n",
        "    |---->(+)<-----|\n",
        "           |\n",
        "        output\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            data_dim,\n",
        "            net_cfg: OmegaConf,\n",
        "            kernel_cfg: OmegaConf,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Method to init the S4 block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_norm_layer = GetBatchNormalization(data_dim=data_dim, num_features=in_channels)\n",
        "\n",
        "        # separable flexible convolutional layer\n",
        "        self.sep_flex_conv_layer = SepFlexConv(\n",
        "            data_dim=data_dim,\n",
        "            in_channels=in_channels,\n",
        "            net_cfg=net_cfg,\n",
        "            kernel_cfg=kernel_cfg\n",
        "        )\n",
        "\n",
        "        self.gelu_layer = [nn.GELU(), nn.GELU()]\n",
        "\n",
        "        self.dropout_layer = GetDropout(data_dim=data_dim)\n",
        "\n",
        "        # pointwise linear convolutional layer\n",
        "        self.pointwise_linear_layer = LinearLayer(data_dim, in_channels, out_channels)\n",
        "\n",
        "        self.seq_modules = nn.Sequential(\n",
        "            self.batch_norm_layer,\n",
        "            self.sep_flex_conv_layer,\n",
        "            self.gelu_layer[0],\n",
        "            self.dropout_layer,\n",
        "            self.pointwise_linear_layer,\n",
        "            self.gelu_layer[1]\n",
        "        )\n",
        "\n",
        "        # Used in residual networks (ResNets) to add a direct path from the input to the output,\n",
        "        # which helps in training deeper networks by mitigating the vanishing gradient problem.\n",
        "        shortcut = []\n",
        "        if in_channels != out_channels:\n",
        "            shortcut.append(LinearLayer(data_dim, in_channels, out_channels))\n",
        "            nn.init.kaiming_normal_(shortcut[0].weight)\n",
        "            if shortcut[0].bias is not None:\n",
        "                shortcut[0].bias.data.fill_(value=0.0)\n",
        "        # If no layer is added (because in_channels and out_channels were the same),\n",
        "        # the shortcut will be empty and effectively be an identity mapping.\n",
        "        self.shortcut = nn.Sequential(*shortcut)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Standard method of nn.modules we embed also the residual connection\n",
        "        \"\"\"\n",
        "        shortcut = self.shortcut(x)\n",
        "        out = self.seq_modules(x)\n",
        "        return out + shortcut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BQf5BMV-uGb"
      },
      "source": [
        "CCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr-R7CZC-uGb"
      },
      "outputs": [],
      "source": [
        "class CCNN(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    CCNN architecture (Romero et al., 2022) as defined in the original paper.\n",
        "\n",
        "    input --> SepFlexConv --> BatchNorm --> GELU --> L x S4Block --> BatchNorm --> GlobalAvgPool -->PointwiseLinear --> output\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        data_dim: int,\n",
        "        cfg: OmegaConf\n",
        "    ):\n",
        "        super(CCNN, self).__init__()\n",
        "\n",
        "        self.no_blocks = cfg.net.no_blocks\n",
        "        hidden_channels = cfg.net.hidden_channels\n",
        "\n",
        "        self.learning_rate = cfg.train.learning_rate\n",
        "        self.warmup_epochs = cfg.train.warmup_epochs\n",
        "        self.epochs = cfg.train.epochs\n",
        "        self.start_factor = cfg.train.start_factor\n",
        "        self.end_factor = cfg.train.end_factor\n",
        "\n",
        "        # separable flexible convolutional layer\n",
        "        self.sep_flex_conv_layer = SepFlexConv(\n",
        "            data_dim=data_dim,\n",
        "            in_channels=in_channels,\n",
        "            net_cfg=cfg.net,\n",
        "            kernel_cfg=cfg.kernel\n",
        "        )\n",
        "        # batch normalization layer\n",
        "        self.batch_norm_layer = [\n",
        "            GetBatchNormalization(data_dim=data_dim, num_features=hidden_channels),\n",
        "            GetBatchNormalization(data_dim=data_dim, num_features=hidden_channels)\n",
        "        ]\n",
        "        # gelu layer\n",
        "        self.gelu_layer = nn.GELU()\n",
        "        # s4blocks\n",
        "        self.blocks = []\n",
        "        for _ in range(self.no_blocks):\n",
        "            s4 = S4Block(in_channels=hidden_channels, out_channels=hidden_channels, data_dim=data_dim, net_cfg=cfg.net, kernel_cfg=cfg.kernel)\n",
        "            self.blocks.append(s4)\n",
        "\n",
        "\n",
        "        # global average pooling layer (the information of each channel is compressed into a single value)\n",
        "        self.global_avg_pool_layer = GetAdaptiveAvgPool(data_dim=data_dim, output_size=(1,) * data_dim)\n",
        "        # pointwise linear convolutional layer\n",
        "        self.pointwise_linear_layer = LinearLayer(data_dim, hidden_channels, out_channels)\n",
        "\n",
        "        # define sequencial modules\n",
        "        self.seq_modules = nn.Sequential(\n",
        "            self.sep_flex_conv_layer,\n",
        "            self.batch_norm_layer[0],\n",
        "            self.gelu_layer,\n",
        "            *self.blocks,\n",
        "            self.batch_norm_layer[1],\n",
        "            self.global_avg_pool_layer,\n",
        "            self.pointwise_linear_layer\n",
        "        )\n",
        "\n",
        "        # define metrics\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=out_channels)\n",
        "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.seq_modules(x)\n",
        "\n",
        "        return out.squeeze()\n",
        "\n",
        "    # Probably works only for sMNIST\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        accuracy = self.accuracy(scores, y)\n",
        "        f1_score = self.f1_score(scores, y)\n",
        "        metrics_dict = {\n",
        "            'train_loss': loss,\n",
        "            'train_accuracy': accuracy,\n",
        "            'train_f1_score': f1_score\n",
        "        }\n",
        "        self.log_dict(dictionary=metrics_dict, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('accuracy', self.accuracy(scores, y))\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, scores, y = self._common_step(batch, batch_idx)\n",
        "\n",
        "        metrics_dict = {\n",
        "            'loss': loss,\n",
        "            'accuracy': self.accuracy(scores, y),\n",
        "\n",
        "        }\n",
        "        self.log_dict(metrics_dict)\n",
        "        #self.log('accuracy', self.accuracy(scores, y))\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
        "        x, y = batch\n",
        "        scores = self.seq_modules(x).squeeze()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        return preds\n",
        "\n",
        "    def _common_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        scores = self.forward(x)\n",
        "        loss = F.cross_entropy(scores, y)\n",
        "        return loss, scores, y\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Define the optimizer (AdamW)\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Define the linear learning rate warm-up for 10 epochs\n",
        "        linear_warmup = optim.lr_scheduler.LinearLR(optimizer=optimizer, start_factor=self.start_factor, end_factor=self.end_factor, total_iters=self.warmup_epochs)\n",
        "\n",
        "        # Define the cosine annealing scheduler\n",
        "        cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(self.epochs - self.warmup_epochs))\n",
        "\n",
        "        # Combine the warm-up and cosine annealing using SequentialLR\n",
        "        scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[linear_warmup, cosine_scheduler], milestones=[self.warmup_epochs])\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r5l_ZDO-uGc"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6ZrxTv_-uGc"
      },
      "outputs": [],
      "source": [
        "def create_model(cfg: OmegaConf) -> CCNN:\n",
        "    return CCNN(\n",
        "        in_channels=cfg.net.in_channels,\n",
        "        out_channels=cfg.net.out_channels,\n",
        "        data_dim=cfg.net.data_dim,\n",
        "        cfg=cfg\n",
        "    )\n",
        "\n",
        "def setup_trainer_components(cfg: OmegaConf):\n",
        "    # Setup logger\n",
        "    logger = None\n",
        "    if cfg.train.logger:\n",
        "        logger = TensorBoardLogger(\"tb_logs\", name=f\"{cfg.data.dataset}_{cfg.net.no_blocks}_{cfg.net.hidden_channels}\")\n",
        "\n",
        "    # Setup callbacks\n",
        "    callbacks = []\n",
        "    if cfg.train.callbacks:\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor=\"val_acc\",\n",
        "            dirpath=\"checkpoints\",\n",
        "            save_top_k=1,\n",
        "            mode=\"max\"\n",
        "        )\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\")\n",
        "        model_summary_callback = ModelSummary(max_depth=-1)\n",
        "        callbacks.extend([model_summary_callback, checkpoint_callback, early_stop_callback])\n",
        "\n",
        "    # Setup profiler\n",
        "    profiler = None\n",
        "    if cfg.train.profiler:\n",
        "        profiler = PyTorchProfiler(\n",
        "            output_filename=\"profiler_output\",\n",
        "            group_by_input_shapes=True,\n",
        "        )\n",
        "\n",
        "    return logger, callbacks, profiler\n",
        "\n",
        "def create_trainer(cfg: OmegaConf, logger: TensorBoardLogger, callbacks: list, profiler: PyTorchProfiler) -> pl.Trainer:\n",
        "    return pl.Trainer(\n",
        "        logger=logger,\n",
        "        accelerator=cfg.train.accelerator,\n",
        "        devices=cfg.train.devices,\n",
        "        max_epochs=cfg.train.epochs,\n",
        "        callbacks=callbacks,\n",
        "        profiler=profiler\n",
        "    )\n",
        "\n",
        "def train_and_evaluate(trainer: pl.Trainer, model: CCNN, datamodule, callbacks: list) -> None:\n",
        "    trainer.fit(model, datamodule)\n",
        "    trainer.validate(model, datamodule)\n",
        "    trainer.test(model, datamodule)\n",
        "    checkpoint_callback = next(cb for cb in callbacks if isinstance(cb, ModelCheckpoint))\n",
        "    print(\"Finished training, best model path: \", checkpoint_callback.best_model_path)\n",
        "\n",
        "def load_and_predict(trainer: pl.Trainer, model: CCNN, datamodule, path: str) -> None:\n",
        "    # TODO check how you take the best model path\n",
        "    model = model.load_from_checkpoint(path)\n",
        "    trainer.predict(model, datamodule)\n",
        "\n",
        "cfg = OmegaConf.load(\"../config/config.yaml\")\n",
        "\n",
        "# 1. Create the dataset\n",
        "datamodule = get_data_module(cfg)\n",
        "# 2. Create the model\n",
        "model = create_model(cfg)\n",
        "# 3. Create the logger, callbacks, profiler and trainer\n",
        "logger, callbacks, profiler = setup_trainer_components(cfg)\n",
        "trainer = create_trainer(cfg, logger, callbacks, profiler)\n",
        "\n",
        "# 4. Train the model or use a pretrained one\n",
        "if not cfg.pre_trained:\n",
        "    train_and_evaluate(trainer, model, datamodule, callbacks)\n",
        "else:\n",
        "    load_and_predict(trainer, model, datamodule, callbacks)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}